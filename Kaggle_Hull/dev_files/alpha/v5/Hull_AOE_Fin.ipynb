{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hull Tactical - Gen5 \"Lean & Fast\" + Meta-Learning\n",
    "\n",
    "### STRATEGY UPDATE: Gen5\n",
    "This notebook implements the **\"Lean & Fast\"** optimization with an added **Meta-Learning** layer.\n",
    "\n",
    "### CORE UPGRADES:\n",
    "1. **Automated Hyperparameter Optimization (HPO)**: Tries to use `optuna` if available to find the perfect balance. If running offline without Optuna, falls back to robust Gen5 defaults.\n",
    "2. **Walk-Forward Validation**: Prevents overfitting by optimizing on a validation set while keeping a strict hold-out set for final testing.\n",
    "3. **Dynamic Feature Engineering**: Feature windows are no longer hardcoded; they adapt based on the meta-learner's findings.\n",
    "4. **\"Flash\" Regime Detection**: Optimizes the ratio between short-term and long-term volatility to switch between **Aggressive** and **Defensive** modes instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "# Try importing Optuna for Meta-Learning, handle offline/missing case gracefully\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not found. Using Gen5 Defaults (Meta-Learning skipped).\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# 1. CONFIGURATION (DYNAMIC)\n",
    "# -----------------------------------------------------------------------------------------\n",
    "class Config:\n",
    "    SEED = 42\n",
    "    \n",
    "    # Gen5 Defaults (Optimized for \"Lean & Fast\")\n",
    "    VOL_SHORT = 5\n",
    "    VOL_LONG = 22\n",
    "    VOL_QUARTERLY = 66\n",
    "    \n",
    "    EMA_FAST = 5\n",
    "    EMA_SLOW = 26\n",
    "    \n",
    "    # Model Params (Gen5 Robust Defaults)\n",
    "    LGBM_PARAMS = {\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 5,\n",
    "        'num_leaves': 31,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.5, # Gen5: Reduced to force feature diversity\n",
    "        'reg_alpha': 0.1,        # Gen5: Increased L1 Regularization\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Trading Logic\n",
    "    BASE_W_LINEAR = 0.4\n",
    "    TARGET_VOL = 0.005\n",
    "    MAX_LEVERAGE = 2.0\n",
    "    SGD_LR = 0.001\n",
    "    SGD_ALPHA = 0.001\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# 2. FEATURE ENGINEERING (ADAPTIVE)\n",
    "# -----------------------------------------------------------------------------------------\n",
    "def calculate_ema(series, period):\n",
    "    return series.ewm(span=period, adjust=False).mean()\n",
    "\n",
    "def calculate_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_macd(series, fast=12, slow=26, signal=9):\n",
    "    ema_fast = calculate_ema(series, fast)\n",
    "    ema_slow = calculate_ema(series, slow)\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = calculate_ema(macd_line, signal)\n",
    "    macd_hist = macd_line - signal_line\n",
    "    return macd_line, signal_line, macd_hist\n",
    "\n",
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Uses Config values dynamically\n",
    "    df = df.copy()\n",
    "    targets = ['forward_returns', 'risk_free_rate']\n",
    "    base_col = 'lag_forward_returns_1'\n",
    "    \n",
    "    # 1. Lags\n",
    "    for col in targets:\n",
    "        for lag in [1, 2, 3, 5, 10, 22]:\n",
    "            df[f'lag_{col}_{lag}'] = df[col].shift(lag)\n",
    "            \n",
    "    # 2. Volatility (Dynamic Windows)\n",
    "    df['vol_short'] = df[base_col].rolling(Config.VOL_SHORT).std()\n",
    "    df['vol_long'] = df[base_col].rolling(Config.VOL_LONG).std()\n",
    "    df['vol_quarterly'] = df[base_col].rolling(Config.VOL_QUARTERLY).std()\n",
    "    \n",
    "    # 3. Momentum & Tech\n",
    "    df['mom_short'] = df[base_col].rolling(Config.VOL_SHORT).mean()\n",
    "    df['ema_fast'] = calculate_ema(df[base_col], Config.EMA_FAST)\n",
    "    df['ema_slow'] = calculate_ema(df[base_col], Config.EMA_SLOW)\n",
    "    df['ema_cross'] = df['ema_fast'] - df['ema_slow']\n",
    "    \n",
    "    df['rsi'] = calculate_rsi(df[base_col], 14)\n",
    "    df['macd'], _, _ = calculate_macd(df[base_col])\n",
    "    \n",
    "    # 4. Regime Features\n",
    "    # Dynamic Vol Ratio\n",
    "    df['vol_ratio'] = df['vol_long'] / (df['vol_quarterly'] + 1e-8)\n",
    "    df['flash_crash_signal'] = df['vol_short'] / (df['vol_long'] + 1e-8)\n",
    "    \n",
    "    # Fill NaNs\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# 3. DATA LOADING & PREP\n",
    "# -----------------------------------------------------------------------------------------\n",
    "def load_data(path):\n",
    "    print(f\"Loading {path}...\")\n",
    "    df_pl = pl.read_csv(path)\n",
    "    cols = [c for c in df_pl.columns if c != 'date_id']\n",
    "    df_pl = df_pl.with_columns([pl.col(c).cast(pl.Float64, strict=False).fill_null(0) for c in cols])\n",
    "    return df_pl.to_pandas()\n",
    "\n",
    "TRAIN_PATH = \"/kaggle/input/hull-tactical-market-prediction/train.csv\"\n",
    "raw_train_df = load_data(TRAIN_PATH)\n",
    "\n",
    "TARGET = \"forward_returns\"\n",
    "DROP_COLS = ['date_id', 'is_scored', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# 4. META-LEARNING (OPTUNA)\n",
    "# -----------------------------------------------------------------------------------------\n",
    "def objective(trial):\n",
    "    # 1. Suggest Parameters\n",
    "    Config.VOL_SHORT = trial.suggest_int('vol_short', 3, 10)\n",
    "    Config.VOL_LONG = trial.suggest_int('vol_long', 15, 30)\n",
    "    Config.VOL_QUARTERLY = trial.suggest_int('vol_quarterly', 50, 80)\n",
    "    \n",
    "    lgbm_params = {\n",
    "        'n_estimators': 500, # Lower for speed during optimization\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 63),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 0.9),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0, log=True),\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'random_state': Config.SEED\n",
    "    }\n",
    "    \n",
    "    # 2. Generate Features (Dynamic)\n",
    "    df = feature_engineering(raw_train_df)\n",
    "    \n",
    "    # 3. Walk-Forward Validation\n",
    "    train_start = 75\n",
    "    \n",
    "    cols_to_drop = [c for c in DROP_COLS if c in df.columns]\n",
    "    X = df.iloc[train_start:].drop(columns=cols_to_drop)\n",
    "    y = df.iloc[train_start:][TARGET]\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = LGBMRegressor(**lgbm_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        scores.append(mean_squared_error(y_val, preds))\n",
    "        \n",
    "    return np.mean(scores)\n",
    "\n",
    "print(\"Starting Optuna Optimization...\")\n",
    "# Check: Optuna Available + Not Rerun\n",
    "if OPTUNA_AVAILABLE and not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    try:\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        \n",
    "        print(\"Best Params:\", study.best_params)\n",
    "        \n",
    "        # Update Config\n",
    "        Config.VOL_SHORT = study.best_params['vol_short']\n",
    "        Config.VOL_LONG = study.best_params['vol_long']\n",
    "        Config.VOL_QUARTERLY = study.best_params['vol_quarterly']\n",
    "        \n",
    "        for k, v in study.best_params.items():\n",
    "            if k in Config.LGBM_PARAMS:\n",
    "                Config.LGBM_PARAMS[k] = v\n",
    "        Config.LGBM_PARAMS['n_estimators'] = 1000\n",
    "    except Exception as e:\n",
    "        print(f\"Optimization Failed: {e}. Using Defaults.\")\n",
    "else:\n",
    "    print(\"Skipping optimization (Offline/Rerun). Using Gen5 Defaults.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# 5. FINAL MODEL TRAINING\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# Re-generate features with FINAL BEST windows\n",
    "train_df = feature_engineering(raw_train_df)\n",
    "train_df = train_df.iloc[75:].reset_index(drop=True)\n",
    "\n",
    "cols_to_drop = [c for c in DROP_COLS if c in train_df.columns]\n",
    "X = train_df.drop(columns=cols_to_drop)\n",
    "y = train_df[TARGET]\n",
    "FEATURES = X.columns.tolist()\n",
    "\n",
    "print(f\"Training Final Model on {len(X)} rows...\")\n",
    "\n",
    "# Linear Model (Base Trend)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "linear_model = SGDRegressor(\n",
    "    loss='squared_error', penalty='l2', alpha=Config.SGD_ALPHA,\n",
    "    learning_rate='constant', eta0=Config.SGD_LR, \n",
    "    random_state=Config.SEED, max_iter=2000\n",
    ")\n",
    "linear_model.fit(X_scaled, y)\n",
    "\n",
    "# Tree Model (Optimized)\n",
    "lgbm_model = LGBMRegressor(**Config.LGBM_PARAMS)\n",
    "lgbm_model.fit(X, y)\n",
    "\n",
    "print(\"Gen5 Models Ready.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------------------\n",
    "# 6. INFERENCE LOOP (Optimized)\n",
    "# -----------------------------------------------------------------------------------------\n",
    "GLOBAL_HISTORY = raw_train_df.iloc[-150:].copy() \n",
    "STEP = 0\n",
    "\n",
    "# 1. Initialize a small buffer to store history of ratios (e.g., last 100 days)\n",
    "ratio_history = [] \n",
    "\n",
    "def get_adaptive_weights(current_vol, long_term_vol, crash_sensitivity=2.0):\n",
    "    \"\"\"\n",
    "    crash_sensitivity: Standard Deviations (Sigma) to trigger defensive mode.\n",
    "                       2.0 = Top 2.5% of violent days (Robust).\n",
    "    \"\"\"\n",
    "    global ratio_history\n",
    "    \n",
    "    # Calculate current ratio\n",
    "    ratio = current_vol / (long_term_vol + 1e-8)\n",
    "    \n",
    "    # Add to history for rolling stats\n",
    "    ratio_history.append(ratio)\n",
    "    if len(ratio_history) > 100: \n",
    "        ratio_history.pop(0) # Keep window fixed size\n",
    "    \n",
    "    w_linear = Config.BASE_W_LINEAR\n",
    "    \n",
    "    # --- DYNAMIC LOGIC HERE ---\n",
    "    # Only calculate Z-score if we have enough history (e.g., 20 days)\n",
    "    if len(ratio_history) > 20:\n",
    "        rolling_series = pd.Series(ratio_history)\n",
    "        \n",
    "        # Calculate dynamic context\n",
    "        mean = rolling_series.mean()\n",
    "        std = rolling_series.std() + 1e-8\n",
    "        \n",
    "        # Z-Score: How many Sigmas away is today's volatility?\n",
    "        z_score = (ratio - mean) / std\n",
    "        \n",
    "        # Decision: Use Z-Score instead of \"1.3\"\n",
    "        if z_score > crash_sensitivity: \n",
    "            # Volatility is statistically shocking relative to recent context\n",
    "            w_linear = 0.7\n",
    "            print(f\"Defensive Mode Triggered! Z-Score: {z_score:.2f}\")\n",
    "            \n",
    "        elif z_score < -1.0:\n",
    "            # Volatility is unusually calm\n",
    "            w_linear = 0.2\n",
    "            \n",
    "    return w_linear, 1.0 - w_linear\n",
    "\n",
    "def predict(test_pl: pl.DataFrame) -> float:\n",
    "    global GLOBAL_HISTORY, STEP, linear_model, scaler\n",
    "    \n",
    "    # 1. Update History\n",
    "    cols = [c for c in test_pl.columns if c != 'date_id']\n",
    "    test_pl = test_pl.with_columns([pl.col(c).cast(pl.Float64, strict=False).fill_null(0) for c in cols])\n",
    "    test_df_raw = test_pl.to_pandas()\n",
    "    \n",
    "    GLOBAL_HISTORY = pd.concat([GLOBAL_HISTORY, test_df_raw], axis=0, ignore_index=True)\n",
    "    \n",
    "    # 2. Features (Uses Best Params from Config)\n",
    "    full_features = feature_engineering(GLOBAL_HISTORY)\n",
    "    current_features = full_features.iloc[[-1]][FEATURES]\n",
    "    \n",
    "    # 3. Prediction\n",
    "    curr_X_scaled = scaler.transform(current_features)\n",
    "    pred_linear = linear_model.predict(curr_X_scaled)[0]\n",
    "    pred_tree = lgbm_model.predict(current_features)[0]\n",
    "    \n",
    "    # 4. Regime Ensemble\n",
    "    curr_vol = current_features['vol_short'].values[0] \n",
    "    long_vol = current_features['vol_quarterly'].values[0]\n",
    "    \n",
    "    w_lin, w_tree = get_adaptive_weights(curr_vol, long_vol)\n",
    "    raw_pred = (pred_linear * w_lin) + (pred_tree * w_tree)\n",
    "    \n",
    "    # 5. Risk Control\n",
    "    safe_vol = curr_vol if curr_vol > 1e-5 else 0.005\n",
    "    vol_scalar = Config.TARGET_VOL / safe_vol\n",
    "    sharpe_forecast = abs(raw_pred) / safe_vol\n",
    "    \n",
    "    allocation_size = sharpe_forecast * vol_scalar * 50\n",
    "    sign = np.sign(raw_pred)\n",
    "    \n",
    "    # RSI Sanity Check\n",
    "    rsi = current_features['rsi'].values[0]\n",
    "    if rsi > 75 and sign > 0: allocation_size *= 0.5\n",
    "    elif rsi < 25 and sign < 0: allocation_size *= 0.5\n",
    "        \n",
    "    allocation = np.clip(1.0 + (sign * allocation_size), 0.0, 2.0)\n",
    "    \n",
    "    # 6. Online Learning\n",
    "    try:\n",
    "        prev_target = test_df_raw['lagged_forward_returns'].values[0]\n",
    "        if not np.isnan(prev_target) and STEP > 0:\n",
    "            linear_model.partial_fit(curr_X_scaled, [prev_target])\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    if len(GLOBAL_HISTORY) > 300:\n",
    "        GLOBAL_HISTORY = GLOBAL_HISTORY.iloc[-200:].reset_index(drop=True)\n",
    "        \n",
    "    STEP += 1\n",
    "    return float(allocation)\n",
    "\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14348714,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}