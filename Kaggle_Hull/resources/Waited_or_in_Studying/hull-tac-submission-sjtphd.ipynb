{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":111543,"databundleVersionId":14861981},{"sourceType":"datasetVersion","sourceId":14162734,"datasetId":8950501,"databundleVersionId":14952646},{"sourceType":"datasetVersion","sourceId":14062252,"datasetId":8950519,"databundleVersionId":14842344},{"sourceType":"kernelVersion","sourceId":286178393}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Overview\n\nSince the final decision variable in this competition is the **position size** $p_t$, the problem naturally fits into a **predict-then-optimize** framework: first extract as much *useful probabilistic information* as possible from data, and then design a decision rule that is *explicitly aligned with the competition score*.\n\nMy solution consists of three tightly coupled components.\n\n### 1) Probabilistic forecasting\n\nInstead of relying on a single point prediction, I train a family of machine learning regression models to characterize the *conditional distribution* of market excess returns. Concretely, the models are used to predict:\n\n- The conditional mean of excess return  \n- The conditional variance  \n- Multiple left-tail quantiles (used to approximate CVaR and other tail risk measures)\n\nThe motivation is straightforward: in an extremely low signal-to-noise environment, point forecasts alone are fragile, while distributional information provides much richer signals for downstream risk-aware decision making.\n\n### 2) Single-step position optimization\n\nThe official competition score is defined over a rolling time window and involves non-linear, non-convex statistics (Sharpe ratio, volatility ratios, geometric means). Directly optimizing this window-based score is impractical.\n\nInstead, I derive a **single-time-step surrogate optimization problem** that approximates the dominant behavior of the score. The resulting objective explicitly incorporates:\n\n- Variance penalty (Sharpe-style risk control)  \n- CVaR penalty (expected loss in the worst tail)  \n- Tail variance penalty (via conditional second moments)  \n- Multi-order turnover penalties to suppress excessive trading  \n- Explicit penalties aligned with the competition rules $\\rho_1$ and $\\rho_2$\n\nThis step is where most of the mathematical derivation effort is spent: the goal is to translate a window-level evaluation metric into a tractable, stable, single-step decision problem.\n\n### 3) Ensembling\n\nIn the final stage, I ensemble both **models** and **strategies**:\n\n- Multiple ML models (LightGBM, CatBoost, XGBoost)  \n- Two types of trading logic:\n  - A simple signal-based position rule （as the demo submission notebook provided) \n  - The derived optimization-based position policy\n\nThe ensemble weights are tuned using walk-forward validation to balance robustness and performance.\n### 4) Hyperparameter Tuning\n\nAll penalty weights, CVaR confidence levels, turnover orders, and ensemble coefficients are selected via **walk-forward hyperparameter search**, ensuring that the final solution is not only high-scoring but also stable across time.","metadata":{"_uuid":"9d7e6894-7b3b-4510-aa7e-b15dc5e5d43c","_cell_guid":"87fb4455-1fba-454e-9121-0329d77b81e0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Problem Formulation\n\nAt trading day $t$, we define:\n\n- Position: $p_t$  \n- Market return: $r_t^m$  \n- Risk-free rate: $r_t^f$  \n- Market excess return: $e_t^m = r_t^m - r_t^f$\n\nThen the strategy return is\n$$\nr_t^s = r_t^f(1 - p_t) + p_t r_t^m = p_t e_t^m + r_t^f,\n$$\nand the strategy excess return is\n$$\ne_t^s = r_t^s - r_t^f = p_t e_t^m.\n$$\n\nThe Sharpe ratio (the most important metric in this competition) is defined as\n\n$$\\text{Sharp}=\n\\frac{\n\\left(\\prod_{t=1}^{T}(1+e_t^s)\\right)^{\\frac{1}{T}} - 1\n}{\n\\operatorname{std}(\\{r_t^s\\}_{t=1}^{T})\n}\n\\sqrt{252}.\n$$\nThat is, annualized mean excess return divided by the standard deviation of daily returns.\n\nThe final competition score applies two penalties to the Sharpe ratio:\n\n- **Volatility penalty**: if annualized strategy volatility exceeds $1.2\\times$ market volatility, the excess part is penalized linearly:\n  $$\n \\rho_1=1+\\max(0,\\frac{std(\\{r_t^s\\})_{t=1}^T}{std(\\{r_t^m\\})_{t=1}^T}-1.2)\n  $$\n\n- **Underperformance penalty**: the strategy cannot “sit in cash and earn interest” while consistently underperforming the market:  \n  $$\\rho_2=\n  1 + 0.01 \\max\\!\\left(\n  0,\n  \\left[\n  \\left(\\prod_{t=1}^{T}(1+e_t^m)\\right)^{\\frac{1}{T}}-\n  \\left(\\prod_{t=1}^{T}(1+e_t^s)\\right)^{\\frac{1}{T}}\n  \\right]\n  \\cdot 252 \\cdot 100\n  \\right).\n  $$\n\nThe final score is\n\n$$\n\\text{Score}=\n\\frac{1}{\\rho_1}\\cdot\\frac{1}{\\rho_2}\\cdot\\text{Sharpe}.\n$$\n\nFrom these equations, we can see that $e_t^m$ is a random variable, while $p_t$ is the decision variable.  \nThe score is mainly driven by $\\{e_t^s\\}_{t=1}^{T}$ and $\\operatorname{std}(\\{r_t^s\\}_{t=1}^{T})$: we want **high returns with low variance**.\n\n---\n\n## Probabilistic Forecasting\n\nI didn’t spend excessive effort on the forecasting pipeline, mainly because the explanatory power of features severely limits the upper bound of predictability. Also, my domain knowledge of financial markets is limited, so the forecasting part is intentionally kept lightweight and robust.\n\nThe core idea here is **not** to chase a highly accurate point forecast, but to extract as much *distributional information* as possible from weak signals, which is far more useful for downstream risk-aware decision making.\n\n\n### Feature Engineering\n\n- Log-mapping of `date_id` to capture long-term trends  \n- Fourier encodings with periods of 5, 21, 63, and 252 trading days to model multi-scale seasonality  \n- Missing indicators + dummy values for early macro and sentiment signals  \n- Removal of highly collinear features to stabilize tree-based models  \n\n### Models and Targets\n\nLet $e_t^m$ denote the market excess return. Instead of modeling only its conditional mean, I decompose the forecasting task into **multiple regression problems** targeting different aspects of the conditional distribution:\n\n1. **Conditional mean**\n   \n   A standard regression model is trained with MAE loss:\n   $$\n   \\mu_t \\;=\\; \\mathbb{E}[e_t^m \\mid \\mathcal{F}_t],\n   $$\n   where $\\mathcal{F}_t$ denotes the available features at time $t$.\n\n2. **Conditional variance**\n   \n   After fitting the mean model, I compute residuals\n   $$\n   \\varepsilon_t = e_t^m - \\mu_t,\n   $$\n   and train a second model to predict the conditional variance:\n   $$\n   \\sigma_t^2 \\;\\approx\\; \\mathbb{E}[\\varepsilon_t^2 \\mid \\mathcal{F}_t].\n   $$\n\n   This explicitly separates *directional signal* from *uncertainty magnitude*.\n\n3. **Tail quantiles (for CVaR estimation)**\n   \n   Multiple quantile regression models are trained for tail levels $\\alpha_1,\\ldots,\\alpha_M$:\n   $$\n   q_{\\alpha,t} \\;=\\; \\inf\\{x:\\mathbb{P}(e_t^m \\le x \\mid \\mathcal{F}_t)\\ge \\alpha\\}.\n   $$\n\nOverall, this setup aims to **maximize the utility of weak predictability**:  \nimproving feature explainability on one hand, and enriching the target’s distributional description on the other.\n\n### Ensemble\n\nAll regression tasks (mean, variance, and quantiles) are implemented using an ensemble of:\n\n- LightGBM  \n- CatBoost  \n- XGBoost  \n\nThese models are chosen mainly for convenience and robustness, especially their ability to handle missing values natively. For simplicity and stability, I use the **same ensemble weights across all quantile levels**, which worked well in practice and avoided unnecessary degrees of freedom.\n\n\n---\n\n## Optimization\n\n### 1) Pure return maximization\n\nIf we only consider maximizing expected excess return, the single-step problem is\n\n$$\n\\max_{p_t}\\; \\mathbb{E}[e_t^s]=\n\\max_{p_t}\\; p_t \\mu_t,\n$$\nwhere $\\mu_t = \\mathbb{E}[e_t^m]$ is the predicted mean excess return.\n\nBecause the objective is linear in $p_t$, taking the derivative implies a boundary solution:\n- If $\\mu_t < 0$, choose $p_t = 0$ (do not trade).\n- If $\\mu_t > 0$, push $p_t$ to the upper bound (e.g., $p_t = 2$).\n\nThis explains why naive “maximize return” sizing tends to produce extreme positions (either no trade or all-in).\n\n\n### 2) Variance-penalized objective (Sharpe-style sizing)\n\nIntroducing variance penalty yields a Sharpe-like surrogate:\n$$\n\\max_{p_t}\\;\np_t \\mu_t - \\lambda_{\\text{var}} p_t^2 \\sigma_t^2,\n$$\nwhere $\\sigma_t^2 = \\operatorname{Var}(e_t^m)$ is the predicted conditional variance.\n\nNow the decision variable appears as a quadratic term, and the unconstrained optimum becomes\n$$\np_t^\\star = \\frac{\\mu_t}{2\\lambda_{\\text{var}}\\sigma_t^2}.\n$$\n\nCompared to pure return maximization, this already prevents extreme leverage and yields smoother position sizing.\n\n\n### 3) CVaR, tail variance, and turnover penalties\n\nBuilding on the variance-penalized formulation, I further incorporate a downside tail risk penalty (CVaR), a tail “volatility” penalty (via conditional second moments in the left tail), and higher-order lagged turnover penalties to smooth trading dynamics. The practical single-step objective is:\n$$\n\\max_{p_t}\\;\np_t \\mu_t-\n\\lambda_{\\text{var}} p_t^2 \\sigma_t^2-\n\\lambda_{\\text{cvar}} p_t \\,\\text{CVaR}_\\alpha(e_t^m)\n$$\n\n$$\n-\\lambda_{\\text{tail}} p_t^2 \\mathbb{E}\\!\\left[(e_t^m)^2 \\mid e_t^m \\le q_\\alpha\\right]\n-\\lambda_k |p_t - \\frac{1}{K}\\sum_{k=1}^{K} p_{t-k}|.\n$$\n\nHere $q_\\alpha$ is the (left-tail) $\\alpha$-quantile of $e_t^m$ (i.e., VaR at level $\\alpha$), and the CVaR term is defined as the conditional expectation in the worst $\\alpha$ fraction of outcomes:\n$$\n\\text{CVaR}_\\alpha(e_t^m)=\\mathbb{E}\\!\\left[e_t^m \\mid e_t^m \\le q_\\alpha\\right].\n$$\nIn practice, I estimate $q_\\alpha$ and $\\text{CVaR}_\\alpha$ from my **multiple quantile regression models**. Suppose the models output conditional quantile forecasts $\\hat q_{\\beta,t}\\approx q_\\beta(e_t^m\\mid \\mathcal{F}_t)$ for a grid of tail levels $\\beta\\in(0,\\alpha]$ (e.g., several small quantiles up to $\\alpha$). Then I use the standard identity that CVaR can be written as an average of tail quantiles:\n$$\n\\text{CVaR}_\\alpha(X)=\\frac{1}{\\alpha}\\int_{0}^{\\alpha} q_u(X)\\,du,\n$$\nand approximate it numerically using the predicted quantiles:\n$$\n\\widehat{\\text{CVaR}}_{\\alpha,t}\\approx \\frac{1}{\\alpha}\\sum_{j=1}^{M}\\hat q_{\\beta_j,t}\\,\\Delta\\beta_j,\n\\quad \\Delta\\beta_j=\\beta_j-\\beta_{j-1},\\ \\beta_M=\\alpha.\n$$\n\nSimilarly, the tail variance proxy is implemented via the conditional second moment in the left tail. Using the same quantile-grid approximation,\n$$\n\\mathbb{E}\\!\\left[X^2\\mid X\\le q_\\alpha\\right]=\\frac{1}{\\alpha}\\int_{0}^{\\alpha} q_u(X)^2\\,du\n\\ \\Rightarrow\\\n\\widehat{m}_{2,\\alpha,t}\\approx \\frac{1}{\\alpha}\\sum_{j=1}^{M}\\hat q_{\\beta_j,t}^2\\,\\Delta\\beta_j.\n$$\n\n\nOverall, these tail-aware penalties (CVaR + tail second moment) prevent the optimizer from taking aggressive positions when the predicted downside tail is heavy, while the turnover terms suppress “chasing noise”. Together, they substantially stabilize the position path and improve realized Sharpe under short evaluation windows.\n\n\n\n### 4) Deriving step-wise penalties from $\\rho_1$ and $\\rho_2$\n\nEven though $\\rho_1$ and $\\rho_2$ are window-based and non-convex (ratios of std, geometric means), we can derive simple single-step hinge penalties that mimic their dominant effects.\n\n#### (a) Volatility penalty $\\rho_1$\n\nRecall\n$$\n\\rho\n1 + \\max\\!\\left(\n0,\n\\frac{\\operatorname{std}(\\{r_t^s\\}_{t=1}^{T})}\n     {\\operatorname{std}(\\{r_t^m\\}_{t=1}^{T})}-1.2\n\\right).\n$$\n\nSince $r_t^s = r_t^f + p_t e_t^m$ and $r_t^f$ is approximately constant within the window,\n$$\n\\frac{\\operatorname{std}(\\{r_t^s\\})}{\\operatorname{std}(\\{r_t^m\\})}\n\\approx\n\\frac{\\operatorname{std}(\\{p_t e_t^m\\})}{\\operatorname{std}(\\{e_t^m\\})}.\n$$\n\nLet $x_t = e_t^m$ and $w_t = p_t$. Using\n$$\n\\operatorname{std}^2(w x) = \\mathbb{E}[w^2 x^2] - \\big(\\mathbb{E}[w x]\\big)^2,\n$$\nand the usual daily-return approximation that $\\mathbb{E}[x]$ is small (so $\\big(\\mathbb{E}[w x]\\big)^2$ is second-order), we take\n$$\n\\operatorname{std}^2(w x) \\approx \\mathbb{E}[w^2 x^2].\n$$\n\nIf $w_t$ varies slowly and is weakly correlated with $x_t^2$ in the window,\n$$\n\\mathbb{E}[w^2 x^2] \\approx \\mathbb{E}[w^2]\\mathbb{E}[x^2].\n$$\n\nTherefore,\n$$\n\\operatorname{std}(p_t e_t^m)\n\\approx\n\\sqrt{\\mathbb{E}[p_t^2]}\\sqrt{\\mathbb{E}[(e_t^m)^2]},\n\\quad\n\\operatorname{std}(e_t^m)\\approx \\sqrt{\\mathbb{E}[(e_t^m)^2]},\n$$\nso the ratio becomes\n$$\n\\frac{\\operatorname{std}(r_t^s)}{\\operatorname{std}(r_t^m)}\n\\approx\n\\sqrt{\\mathbb{E}[p_t^2]}.\n$$\n\nThis shows $\\rho_1$ is essentially penalizing the RMS position exceeding $1.2$.  \nA conservative single-step surrogate is to softly penalize positions above $1.2$:\n$$\n\\text{pen}_1(p_t) = \\lambda_1\\max(p_t-1.2,0).\n$$\n\n\n#### (b) Underperformance penalty $\\rho_2$\n\n\nRecall\n$$\n\\rho_2=\n1+0.01 \\max\\!\\left(\n0,\n\\left[\n\\left(\\prod_{t=1}^{T}(1+e_t^m)\\right)^{\\frac{1}{T}}\n\\left(\\prod_{t=1}^{T}(1+e_t^s)\\right)^{\\frac{1}{T}}\n\\right]\n\\cdot 252 \\cdot 100\n\\right),\n$$\nwhere $e_t^s = p_t e_t^m$.\n\nDefine geometric mean growth rates\n$$\nG_m = \\left(\\prod_{t=1}^{T}(1+e_t^m)\\right)^{\\frac{1}{T}},\n\\quad\nG_s = \\left(\\prod_{t=1}^{T}(1+e_t^s)\\right)^{\\frac{1}{T}}.\n$$\n\nUse log transform:\n$$\n\\log G = \\frac{1}{T}\\sum_{t=1}^{T}\\log(1+e_t).\n$$\n\nThen\n$$\n\\Delta := \\log G_m - \\log G_s=\n\\frac{1}{T}\\sum_{t=1}^{T}\\left[\\log(1+e_t^m)-\\log(1+p_t e_t^m)\\right].\n$$\n\nFor daily returns, $|e_t^m|\\ll 1$, so apply the Taylor expansion\n$$\n\\log(1+z)=z-\\frac{z^2}{2}+O(z^3).\n$$\n\nThus\n$$\n\\log(1+e_t^m)-\\log(1+p_t e_t^m)\n\\approx\n(1-p_t)e_t^m-\\frac{1-p_t^2}{2}(e_t^m)^2,\n$$\nand\n$$\n\\Delta\n\\approx\n\\frac{1}{T}\\sum_{t=1}^{T}(1-p_t)e_t^m-\n\\frac{1}{2T}\\sum_{t=1}^{T}(1-p_t^2)(e_t^m)^2.\n$$\n\nThe first term is first-order in $e_t^m$ and usually dominates at daily frequency.  \nAssuming the market has positive long-run excess returns (so the window-average of $e_t^m$ is positive), the sign of the dominant term is mainly controlled by $(1-p_t)$:\n\n- If $p_t<1$, then $(1-p_t)>0$, so $\\Delta>0$ and $G_m>G_s$ (strategy underperforms market)  \n- If $p_t>1$, then $(1-p_t)<0$, so $\\Delta<0$ and typically $G_s\\ge G_m$ (no underperformance)\n\nFinally, map back from log space to level space. Since $G=\\exp(\\log G)$, for small $\\Delta$,\n$$\nG_m - G_s \\approx G_s\\,(e^\\Delta-1)\\approx G_s\\,\\Delta.\n$$\n\nAll constants (including $0.01\\times252\\times100$ and the scale of $G_s$) can be absorbed into a tunable weight, giving a practical single-step surrogate that penalizes under-investment:\n$$\n\\text{pen}_2(p_t)=\\lambda_2\\max(1-p_t,0).\n$$\n\n#### c) Final single-step penalty \n\nPutting the two surrogates together, the final single-step penalty is\n$$\n\\lambda_{1}\\max(p_t-1.2,0)+\\lambda_{2}\\max(1-p_t,0).\n$$\n\n### Final single-step optimization Problem\nCombining all components, the final single-step optimization problem is\n$$\n\\max_{p_t} \\mathcal{J}(p_t)=\np_t \\mu_t\n-\\lambda_{\\text{var}} p_t^2 \\sigma_t^2\n-\\lambda_{\\text{cvar}} p_t \\,\\widehat{\\text{CVaR}}_{\\alpha,t}\n-\\lambda_{\\text{tail}} p_t^2 \\widehat{m}_{2,\\alpha,t}\n$$\n\n$$\n-\\lambda_k |p_t - \\frac{1}{K}\\sum_{k=1}^{K} p_{t-k}|\n-\\lambda_{1}\\max(p_t-1.2,0)\n-\\lambda_{2}\\max(1-p_t,0).\n$$\n\nThe full objective becomes non-convex but is still a univariate optimization problem in $p_t$.  \nUsing Big-M + MIP solvers would be overkill, so I simply used grid search, which is robust and fast in practice.\n\n\n\n\n\n---\n\n## Hyperparameter Search\n\nThe derivation maps a window-based score into a single-step optimization formulation, but many hyperparameters remain to be tuned (e.g., penalty weights, CVaR confidence level, turnover order, etc.).\n\nSince lagged features are used, I applied walk-forward evaluation and optimized the following stability-aware objective:\n$$\n\\max_{\\theta}\\left(\n\\mathbb{E}_{\\text{folds}}[\\text{Score}(\\theta)]-\n\\gamma\\cdot\n\\max\\!\\left(\n\\operatorname{Var}_{\\text{folds}}[\\text{Score}(\\theta)],\n\\;\\tau\n\\right)\n\\right),\n$$\nwhere $\\theta$ denotes hyperparameters, $\\gamma$ controls the strength of stability regularization, and $\\tau$ is a variance floor to avoid overly small variance estimates dominating the objective.\n\nThis explicitly trades off high average score and low variance across folds.","metadata":{"_uuid":"02e87ab5-e01c-4f48-b5ba-e8acb31cd6cb","_cell_guid":"86bd454f-3a5f-4e53-b303-a2873863fbbc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Evaluation Results\n\nWalk-forward backtesting was conducted on the public training set provided by the competition. The initial training window length was 5,186 observations, and the validation window covered 125 trading days. The final version of the solution achieved the following results across all folds:\n\n| fold | train_start_idx | train_end_idx | val_start_idx | val_end_idx | sharpe  | vol_penalty | return_penalty | score    |\n|------|------------------|--------------|---------------|------------|---------|-------------|----------------|----------|\n| 0    | 0                | 5185         | 5186          | 5310       | 2.20394 | 1.00000     | 1.35662        | 1.62458  |\n| 1    | 0                | 5310         | 5311          | 5435       | 0.65284 | 1.00000     | 1.00000        | 0.65284  |\n| 2    | 0                | 5435         | 5436          | 5560       | 0.44624 | 1.17504     | 1.00000        | 0.37977  |\n| 3    | 0                | 5560         | 5561          | 5685       | 0.98497 | 1.00000     | 1.00000        | 0.98497  |\n| 4    | 0                | 5685         | 5686          | 5810       | 2.17824 | 1.00000     | 1.00001        | 2.17821  |\n| 5    | 0                | 5810         | 5811          | 5935       | 2.58647 | 1.00000     | 1.05382        | 2.45438  |\n| 6    | 0                | 5935         | 5936          | 6060       | 2.19054 | 1.00000     | 1.00000        | 2.19054  |\n| 7    | 0                | 6060         | 6061          | 6185       | 1.34628 | 1.00000     | 1.00000        | 1.34628  |\n| 8    | 0                | 6185         | 6186          | 6310       | 1.01225 | 1.19272     | 1.00000        | 0.84869  |\n| 9    | 0                | 6310         | 6311          | 6435       | 1.66722 | 1.00000     | 1.00000        | 1.66722  |\n| 10   | 0                | 6435         | 6436          | 6560       | -0.67137| 1.26973     | 1.00933        | -0.52386 |\n| 11   | 0                | 6560         | 6561          | 6685       | 1.46267 | 1.00000     | 1.09990        | 1.32981  |\n| 12   | 0                | 6685         | 6686          | 6810       | 1.88504 | 1.00000     | 1.00000        | 1.88504  |\n| 13   | 0                | 6810         | 6811          | 6935       | 2.74985 | 1.00000     | 1.00000        | 2.74985  |\n| 14   | 0                | 6935         | 6936          | 7060       | 3.37012 | 1.00000     | 2.00316        | 1.68240  |\n| 15   | 0                | 7060         | 7061          | 7185       | 0.69500 | 1.00000     | 1.00000        | 0.69500  |\n| 16   | 0                | 7185         | 7186          | 7310       | -0.96679| 1.33497     | 1.61720        | -0.44781 |\n| 17   | 0                | 7310         | 7311          | 7435       | 2.49966 | 1.00000     | 1.00000        | 2.49966  |\n| 18   | 0                | 7435         | 7436          | 7560       | 1.46091 | 1.13292     | 1.00000        | 1.28951  |\n| 19   | 0                | 7560         | 7561          | 7685       | 0.26140 | 1.38698     | 1.00000        | 0.18847  |\n| 20   | 0                | 7685         | 7686          | 7810       | 1.97577 | 1.10118     | 1.00000        | 1.79423  |\n| 21   | 0                | 7810         | 7811          | 7935       | 2.53332 | 1.00000     | 1.00000        | 2.53332  |\n| 22   | 0                | 7935         | 7936          | 8060       | 2.05840 | 1.20488     | 1.00000        | 1.70838  |\n| 23   | 0                | 8060         | 8061          | 8185       | -1.72920| 1.33011     | 5.65410        | -0.22993 |\n| 24   | 0                | 8185         | 8186          | 8310       | 0.24246 | 1.00000     | 1.00000        | 0.24246  |\n| 25   | 0                | 8310         | 8311          | 8435       | 2.07074 | 1.00000     | 1.00000        | 2.07074  |\n| 26   | 0                | 8435         | 8436          | 8560       | 1.07785 | 1.00000     | 1.00041        | 1.07740  |\n| 27   | 0                | 8560         | 8561          | 8685       | 2.45194 | 1.00000     | 1.00000        | 2.45194  |\n| 28   | 0                | 8685         | 8686          | 8810       | 1.17868 | 1.01147     | 1.00000        | 1.16531  |\n| 29   | 0                | 8810         | 8811          | 8935       | 0.07893 | 1.33028     | 1.00000        | 0.05933  |\n| 30   | 0                | 8935         | 8936          | 9047       | 2.07276 | 1.00000     | 1.00000        | 2.07276  |","metadata":{"_uuid":"2fdaf4e7-2702-455a-a79c-b94f1e51d77d","_cell_guid":"24389893-14ab-4351-90c5-5fb902d2bd06","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os, sys, subprocess, shutil, glob\n\nWHEELS_DIR = \"/kaggle/input/xgb-wheels-builder/wheels\"\nTARGET_DIR = \"/kaggle/working/pydeps\"\n\n# 0) 确认 wheels 在\nassert os.path.isdir(WHEELS_DIR), WHEELS_DIR\nprint(\"Wheel files:\", [os.path.basename(p) for p in glob.glob(os.path.join(WHEELS_DIR, \"*.whl\"))])\n\n# 1) 清理旧的目标目录（最稳）\nif os.path.exists(TARGET_DIR):\n    shutil.rmtree(TARGET_DIR)\nos.makedirs(TARGET_DIR, exist_ok=True)\n\n# 2) 离线安装，强制覆盖\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\",\n    \"--no-index\",\n    \"--find-links\", WHEELS_DIR,\n    \"--target\", TARGET_DIR,\n    \"--upgrade\",           # 强制覆盖已有目录\n    \"--no-deps\",           # 只装 xgboost 本体，避免依赖乱动\n    \"xgboost==3.1.2\",\n    \"-q\"\n])\n\n# 3) 把 TARGET_DIR 放到 sys.path 最前面\nif TARGET_DIR not in sys.path:\n    sys.path.insert(0, TARGET_DIR)","metadata":{"_uuid":"be898560-db8a-4e90-8ce5-7848ca06d3e5","_cell_guid":"a0060496-4850-4325-af6c-30602ae15058","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-21T16:14:34.160828Z","iopub.execute_input":"2025-12-21T16:14:34.16137Z","iopub.status.idle":"2025-12-21T16:14:44.976487Z","shell.execute_reply.started":"2025-12-21T16:14:34.16134Z","shell.execute_reply":"2025-12-21T16:14:44.975454Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"f2bc4823-501e-46c6-9f55-39f3821d05ee","_cell_guid":"1215ad6c-7be0-451e-80f8-ffb279c9432d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n不采用其余文件中的代码，全部代码都放在此文件中，确保单个notebook可以运行。\n线上迁移时记得：\n1. 删除本地path\n2. 删除score函数\n3. 删除本地测试代码\n'''\n\nfrom pathlib import Path\nimport joblib\nimport os\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom dataclasses import dataclass, asdict\nfrom typing import Optional, Dict, List, Any, Callable\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\nimport kaggle_evaluation.default_inference_server\n\n\n#%% ======================PATHS===============================\nDATA_PATH: str= '/kaggle/input/hull-tactical-market-prediction'\nMODEL_DIR: str =  \"/kaggle/input/lgbm4market\" \nFEATURE_DIR: str = \"/kaggle/input/features4market\"\n\n\n#%%=================MODEL CONFIGS=========================\n\n# LightGBM parameters\n@dataclass\nclass LGBMParams:\n    n_estimators: int = 50\n    num_leaves: int =20\n    max_depth: int = 8\n    min_data_in_leaf: int = 800\n    learning_rate: float = 0.02\n    objective: str = 'quantile'\n    alpha:float =0.5\n    l1_regularization: float = 10\n    l2_regularization: float = 5\n    random_state: int = 42\n    verbosity: int = -1\n\n@dataclass\nclass CATBOOSTParams:\n    iterations: int = 100\n    # allow alpha to be injected; loss_function will be constructed in __post_init__\n    alpha: float = 0.5\n    loss_function: Optional[str] = None\n    depth: int = 7\n    learning_rate: float = 0.02\n    l2_leaf_reg: float = 3.0\n    min_data_in_leaf: int = 800\n    random_seed: int = 42\n    verbose: bool=False\n\n    def __post_init__(self):\n        # if loss_function not provided explicitly, build quantile loss using alpha\n        if not self.loss_function:\n            self.loss_function = f'Quantile:alpha={self.alpha}'\n\n@dataclass\nclass XGBParams:\n    n_estimators: int = 100\n    max_depth: int = 8\n    learning_rate: float = 0.03\n    subsample: float = 0.8\n    colsample_bytree: float = 0.8\n    min_child_weight: float = 1.0\n    reg_alpha: float = 0.17\n    reg_lambda: float = 8.0\n\n    # quantile\n    quantile_alpha: float = 0.5\n    objective: str = \"reg:quantileerror\"\n\n    # runtime\n    random_state: int = 42\n    n_jobs: int = -1\n    verbosity: int = 0\n\n\n#%%=================STRATEGY CONFIGS=========================\n\n# ============ RETURNS TO SIGNAL CONFIGS ============\nMIN_SIGNAL: float = 0.0                         # Minimum value for the daily signal \nMAX_SIGNAL: float = 2.0                         # Maximum value for the daily signal \n\n#==============NAIVE STRATEGY HYPER-PARAMETERS=================\nMULTIPLIER_Q50: float = 139.36               # Multiplier for scaling the signal in naive strategy\nMULTIPLER_CVAR: float = 0.449          # Multiplier for scaling the CVaR component in naive strategy\n\n#=============CVaR / Tail-Variance strategy hyper-parameters==========\nTAIL_ALPHA: float = 0.2             # 使用下侧 0~alpha 的分位数来估计 CVaR\nTAIL_ALPHA_STEP: float = 0.01        # 分位数步长\n\nLAMBDA_CVAR: float = 0.001            # 尾部平均亏损（CVaR）惩罚权重\nLAMBDA_TAIL_VAR: float = 0.2766       # 尾部方差惩罚权重\nLAMBDA_VARIANCE: float = 0.1486         # 方差惩罚权重\nLAMBDA_TURNOVER: float = 0.01999          # 换手惩罚权重\nLAMBDA_VOL: float = 0.0001                 # 波动率惩罚权重 对应rho1\nLAMBDA_RETURN_PEN: float = 0.00049        # 收益率惩罚权重 对应rho2\nINIT_SIGNAL: float = 0.4               # 初始仓位\nTURNOVER_ORDER: int = 2                # 换手阶数\n\n#============Ensemble===========================\nSTRATEGY_ENSEMBLE_WEIGHT=0.1                 # 组合策略中，naive策略的权重\n\nLGBM_ENSEMBLE_WEIGHT: float = 0.234         # 组合模型权重 0.11608688993930603 \nCATBOOST_ENSEMBLE_WEIGHT: float = 0.6         # 组合模型权重\nXGBOOST_ENSEMBLE_WEIGHT: float = 1-(LGBM_ENSEMBLE_WEIGHT + CATBOOST_ENSEMBLE_WEIGHT)  # 组合模型权重\n\n\n#%% =============Utils Model Functions==================\n\nclass Regressor:\n    \"\"\"\n    Generic regressor wrapper.\n    - model_name: string like \"LGBM\", \"CATBOOST\", or a full class path \"sklearn.ensemble.RandomForestRegressor\"\n    - params: parameter dict for the chosen estimator\n    Provides fit(...) which calls the underlying estimator.fit and save(...) to persist the model\n    \"\"\"\n    def __init__(self, model_name: str, alpha):\n        self.model_name = model_name\n        if model_name==\"LGBM\":\n            self.params = asdict(LGBMParams(alpha=alpha))\n        elif model_name==\"CATBOOST\":\n            # build CATBOOST params, but remove internal-only fields (like 'alpha')\n            params = asdict(CATBOOSTParams(alpha=alpha))\n            # 'alpha' is used to construct loss_function but is not a valid CatBoost param\n            params.pop('alpha', None)\n            self.params = params\n        elif model_name==\"XGBOOST\":\n            self.params = asdict(XGBParams(quantile_alpha=alpha))\n        else:\n            self.params = {}\n        self.model = self._build_model()\n\n    def _build_model(self):\n        name_up = self.model_name.upper()\n        # LightGBM\n        if name_up.startswith(\"LGBM\"):\n            return lgb.LGBMRegressor(**self.params)\n        # CatBoost\n        if name_up.startswith(\"CatBoost\".upper()):\n            return CatBoostRegressor(**self.params)\n        # XGBoost\n        if name_up.startswith(\"XGBOOST\"):\n            params = dict(self.params)\n            params.setdefault(\"enable_categorical\", True)\n            return xgb.XGBRegressor(**params)\n\n    def fit(self, X, y, **fit_kwargs):\n        # Accept polars DataFrame/Series or pandas/numpy\n        X_in = X.to_pandas() if hasattr(X, \"to_pandas\") else X\n        y_in = y.to_pandas() if hasattr(y, \"to_pandas\") else y\n\n        # Ensure the underlying model exists (build it if missing) and satisfy static type checkers\n        if getattr(self, \"model\", None) is None:\n            # Try to (re)build the model if it wasn't created in __init__\n            self.model = self._build_model()\n        assert self.model is not None, \"No underlying model available; check model_name\"\n        self.model.fit(X_in, y_in, **fit_kwargs)\n        return self\n\n    def predict(self, X, **predict_kwargs):\n        \n        if getattr(self, \"model\", None) is None:\n            raise ValueError(\"No model is built. Call fit(...) before predict(...) or ensure model is initialized.\")\n\n        X_in = X.to_pandas() if hasattr(X, \"to_pandas\") else X\n\n        # assign to local variable so type checkers can infer it's not None\n        model = getattr(self, \"model\", None)\n        assert model is not None, \"No underlying model available; check model_name\"\n        preds = model.predict(X_in, **predict_kwargs)\n\n        # If original input was pandas, preserve the index and return a Series\n        try:\n            if isinstance(X, (pd.DataFrame, pd.Series)):\n                return pd.Series(preds, index=X.index)\n        except Exception:\n            pass\n\n        return preds\n\ndef load_model(directory: str= MODEL_DIR, custom_name: str= \"None\") -> Any:\n    \"\"\"\n    Load model by trying known extensions in order.\n    Returns the underlying estimator/booster (not the wrapper).\n    \"\"\"\n\n    # 1) XGBoost json\n    xgb_path = os.path.join(directory, f\"{custom_name}.json\")\n    if os.path.exists(xgb_path):\n        m = xgb.XGBRegressor()\n        m.load_model(xgb_path)\n        return m\n\n    # 2) CatBoost cbm\n    cb_path = os.path.join(directory, f\"{custom_name}.cbm\")\n    if os.path.exists(cb_path):\n        m = CatBoostRegressor()\n        m.load_model(cb_path)\n        return m\n\n    # 3) LightGBM booster txt\n    lgb_path = os.path.join(directory, f\"{custom_name}.txt\")\n    if os.path.exists(lgb_path):\n        return lgb.Booster(model_file=lgb_path)\n\n    # 4) fallback pkl (不推荐，但保底)\n    pkl_path = os.path.join(directory, f\"{custom_name}.pkl\")\n    if os.path.exists(pkl_path):\n        return joblib.load(pkl_path)\n\n    raise FileNotFoundError(f\"Model not found for name={custom_name} in {directory}\")\n\n\n#%% ==============Utils Strategy Classes==================\n\ndef predict_cvar_tail(\n    models_tail: list[Regressor],\n    X_val: pd.DataFrame,\n    tail_alpha: float = TAIL_ALPHA,\n    tail_alpha_step: float = TAIL_ALPHA_STEP,\n) -> dict:\n    \n    n_samples= X_val.shape[0]\n\n    # ---- 多个下侧分位数：估计 CVaR ----\n    alphas = np.arange(tail_alpha_step, tail_alpha + 1e-8, tail_alpha_step)\n    K = len(alphas)\n\n    q_preds = np.zeros((n_samples, K), dtype=float)\n    for j, a in enumerate(alphas):\n        q_preds[:, j] = models_tail[j].predict(X_val)\n\n    # CVaR_alpha^r ≈ (1/K) * sum q(τ_k)\n    cvar_r = q_preds.mean(axis=1)  # shape: (n_samples,)\n\n    # 尾部二阶矩 & 方差\n    tail_second_moment = (q_preds ** 2).mean(axis=1)\n    sigma_tail_sq = tail_second_moment - cvar_r ** 2\n    sigma_tail_sq = np.maximum(sigma_tail_sq, 0.0)\n\n    results_df={\n        'cvar_r': cvar_r,\n        'sigma_tail_sq': sigma_tail_sq,\n    }\n\n    return results_df\n\ndef predict_variance(\n    model_var: Regressor,\n    X_val: pd.DataFrame,\n) -> pd.Series:\n    \n    preds = model_var.predict(X_val)\n    vars= np.exp(preds) - 1e-6\n    var_preds = np.maximum(vars, 0.0)\n    \n    return pd.Series(var_preds)\n\ndef F_w(\n    w: np.ndarray, # 仓位决策\n    mu: float,\n    variance:float,\n    cvar:float,\n    tail_var: float,\n    w_prev_mean: float,\n    lambda_variance: float,\n    lambda_cvar: float,\n    lambda_tail_var: float,\n    lambda_turnover: float,\n    lambda_vol: float,\n    lambda_return_pen: float,\n) -> np.ndarray:\n\n    w = np.asarray(w, dtype=float)\n\n    #收益部分\n    r_strategy= mu * w\n\n    #方差惩罚\n    p_variance = lambda_variance * variance * (w**2)\n\n    # CVaR 惩罚\n    p_cvar = -lambda_cvar * cvar * w\n\n    # 尾部方差惩罚\n    p_tail_var = lambda_tail_var * tail_var * (w**2)\n\n    # 换手惩罚\n    p_turnover = lambda_turnover * (w - w_prev_mean)**2\n\n    # 波动性>市场惩罚\n    p_vol= lambda_vol * np.maximum(0.0, w-1.2)\n\n    # 收益率惩罚\n    p_return_pen = lambda_return_pen * np.maximum(0.0, 1.0 - w)**2\n    \n    return r_strategy - p_variance - p_cvar - p_tail_var - p_turnover - p_vol - p_return_pen\n\ndef maximize_F_grid(\n    F_w_func,              # 函数句柄，比如 F_w\n    *F_args,               # 传给 F_w 的位置参数（除 w 以外的）\n    w_min: float = 0.0,\n    w_max: float = 2.0,\n    n_grid: int = 1001,\n    refine: bool = True,\n    refine_radius: float = 0.02,\n    refine_factor: int = 10,\n    **F_kwargs,            # 传给 F_w 的关键字参数（除 w 以外的）\n):\n    \"\"\"\n    在区间 [w_min, w_max] 上用网格搜索最大化 F(w)。\n\n    参数\n    ----\n    F_w_func : callable\n        目标函数，签名类似 F_w(w, *F_args, **F_kwargs)，w 为 np.ndarray。\n    *F_args, **F_kwargs :\n        传递给 F_w_func 的其余参数（除 w 之外）。\n    w_min, w_max : 搜索区间\n    n_grid : 初始网格点数量\n    refine : 是否在初始最优点附近做二次细化\n    refine_radius : 细化搜索的左右半径\n    refine_factor : 细化阶段相对于 n_grid 的放大倍数\n\n    返回\n    ----\n    best_w : float, 近似最优 w*\n    best_val : float, F(w*) 的近似最大值\n    \"\"\"\n    # ===== 初次粗网格搜索 =====\n    w_grid = np.linspace(w_min, w_max, n_grid)\n    # 关键：直接把参数透传给 F_w_func\n    F_vals = F_w_func(w_grid, *F_args, **F_kwargs)\n\n    idx_best = int(np.argmax(F_vals))\n    best_w = float(w_grid[idx_best])\n    best_val = float(F_vals[idx_best])\n\n    # ===== 可选：在最优点附近再细化一层 =====\n    if refine:\n        left = max(w_min, best_w - refine_radius)\n        right = min(w_max, best_w + refine_radius)\n\n        if right > left:\n            n_refine = max(200, n_grid * refine_factor)\n            w_grid_refine = np.linspace(left, right, n_refine)\n            F_vals_refine = F_w_func(w_grid_refine, *F_args, **F_kwargs)\n\n            idx_best_refine = int(np.argmax(F_vals_refine))\n            best_w_refine = float(w_grid_refine[idx_best_refine])\n            best_val_refine = float(F_vals_refine[idx_best_refine])\n\n            if best_val_refine > best_val:\n                best_w, best_val = best_w_refine, best_val_refine\n\n    return best_w, best_val\n\nclass SignalGenerator:\n\n    def __init__(\n        self,\n        model_q50,\n        models_tail: list,\n        model_var,\n        ensemble_weight: float = 0.5,\n    ):\n        self.model_q50 = model_q50\n        self.models_tail = models_tail\n        self.model_var = model_var\n        self.init_signal = INIT_SIGNAL\n        self.turnover_order = TURNOVER_ORDER\n        self.prev_signals = np.array([self.init_signal] * self.turnover_order,dtype=float)\n        self.ensemble_weight = ensemble_weight\n\n    def call_var_cvar_turnover(self, X_val: pd.DataFrame) -> pd.Series:\n        n_samples = X_val.shape[0]\n\n        mu = self.model_q50.predict(X_val)\n        mu = mu.values if isinstance(mu, pd.Series) else mu\n\n        result_tail = predict_cvar_tail(\n            models_tail=self.models_tail,\n            X_val=X_val,\n            tail_alpha=TAIL_ALPHA,\n            tail_alpha_step=TAIL_ALPHA_STEP\n        )\n        cvar = result_tail['cvar_r']\n        tail_variance = result_tail['sigma_tail_sq']\n\n        variance = predict_variance(\n            model_var=self.model_var,\n            X_val=X_val\n        )\n        variance = variance.values if isinstance(variance, pd.Series) else variance\n\n        signal = np.zeros(n_samples, dtype=float)\n        for i in range(n_samples):\n            w_i, _ = maximize_F_grid(\n                F_w_func=F_w,\n                mu=mu[i],\n                variance=variance[i],\n                cvar=cvar[i],\n                tail_var=tail_variance[i],\n                w_prev_mean=self.prev_signals.mean(),\n                lambda_variance=LAMBDA_VARIANCE,\n                lambda_cvar=LAMBDA_CVAR,\n                lambda_tail_var=LAMBDA_TAIL_VAR,\n                lambda_turnover=LAMBDA_TURNOVER,\n                lambda_vol=LAMBDA_VOL,\n                lambda_return_pen=LAMBDA_RETURN_PEN,\n            )\n            signal[i] = w_i\n\n            self.prev_signals = np.roll(self.prev_signals, -1)\n            self.prev_signals[-1] = w_i\n\n        return pd.Series(signal)\n    \n    def call_naive(self, X_val: pd.DataFrame, multiplier_q50: float = MULTIPLIER_Q50, multipler_cvar: float = MULTIPLER_CVAR) -> pd.Series:\n\n        y_pred_q50 = self.model_q50.predict(X_val)\n        cvar=predict_cvar_tail(\n            models_tail=self.models_tail,\n            X_val=X_val,\n            tail_alpha=TAIL_ALPHA,\n            tail_alpha_step=TAIL_ALPHA_STEP\n        )['cvar_r']\n\n        signal= np.clip(\n            (y_pred_q50) * multiplier_q50 + cvar * multipler_cvar + 1.0\n            , MIN_SIGNAL, MAX_SIGNAL\n        )\n\n        return pd.Series(signal)\n    \n    def call_ensemble(\n        self,\n        X_val: pd.DataFrame) -> pd.Series:\n        signal_cvar = self.call_var_cvar_turnover(X_val)\n        signal_naive = self.call_naive(X_val)\n\n        # 如果两者 index 可能不一致，先对齐（很推荐）\n        signal_cvar, signal_naive = signal_cvar.align(signal_naive, join='inner')\n\n        w = float(self.ensemble_weight)\n\n        cvar = signal_cvar.to_numpy(dtype=np.float64)\n        naive = signal_naive.to_numpy(dtype=np.float64)\n\n        data = (1.0 - w) * cvar + w * naive\n\n        signal = pd.Series(\n            data=data,\n            index=signal_cvar.index,\n            name=\"position\"\n        )\n        \n        return signal\n    \n#%%=================Utils Data Functions=========================\ndef data_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Cleans the DataFrame by handling missing values.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        pd.DataFrame: The cleaned DataFrame.\n    \"\"\"\n    \n    # Convert D1-D9 to categorical\n    cat_cols = [f\"D{i}\" for i in range(1, 10)]\n    for c in cat_cols:\n        if df[c].isna().any():\n            df[c]= df[c].astype(\"category\")\n        else:\n            df[c] = df[c].astype('int64').astype(\"category\")\n\n    return df\n\n\ndef feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Performs feature engineering on the DataFrame.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        ONLINE: bool: Whether in online mode (for inference).\n\n    Returns:\n        pd.DataFrame: The DataFrame with new features.\n    \"\"\"\n    \n    # create U1 and U2 \n    if all(c in df.columns for c in ['I2', 'I1']):\n        df['U1'] = df['I2'] - df['I1']\n    if all(c in df.columns for c in ['M11', 'I2', 'I9', 'I7']):\n        denom = (df['I2'] + df['I9'] + df['I7']) / 3.0\n        # avoid division by zero\n        denom = denom.replace(0, np.nan)\n        df['U2'] = df['M11'] / denom\n\n    # 长期趋势\n    #df[\"t_norm\"] = (df[\"date_id\"] - df[\"date_id\"].min()) / (df[\"date_id\"].max() - df[\"date_id\"].min())\n    df[\"t_log\"] = np.log1p(df[\"date_id\"])\n    \n    \n    # 周期性编码\n    periods = [5, 21, 63, 252]  # 周、月、季度、年\n    for p in periods:\n        df[f\"sin_{p}\"] = np.sin(2 * np.pi * df[\"date_id\"] / p)\n        df[f\"cos_{p}\"] = np.cos(2 * np.pi * df[\"date_id\"] / p)\n    \n    \n    # 盈利指标与市场动量的共振效应\n    if 'E1' in df.columns and 'M1' in df.columns:\n        df['E1_M1_interaction'] = df['E1'] * df['M1']\n    \n    \n    # 对市场宏观和情绪信号设置缺失指示器+哑值\n    df['E7_isna'] = df['E7'].isna().astype(int)\n    df['E7_filled'] = df['E7'].fillna(-1)\n    df['S3_isna'] = df['S3'].isna().astype(int)\n    df['S3_filled'] = df['S3'].fillna(-1)\n\n    # 删除与其它特征共线性强的特征'I5','I9'\n    df = df.drop(columns=['I5', 'I9'], errors='ignore')\n    \n    # 加入lagged 特征\n    # if not ONLINE: #如果是ONLINE则滞后特征自动提供在test DataFrame中\n    #     lagged_features = [\n    #         'forward_returns', 'risk_free_rate', 'target'\n    #     ]\n    #     for feat in lagged_features:\n    #         if feat in df.columns:\n    #             df[f'lagged_{feat}'] = df[feat].shift(1)\n\n    #     if 'lagged_target' in df.columns:\n    #         df = df.rename(columns={'lagged_target': 'lagged_market_forward_excess_returns'})\n\n    #去碎片化\n    df=df.copy()\n    \n    return df\n\ndef load_selected_features(LAGGED:bool=True) -> List[str]:\n    # Load selected features from file and convert to list\n    with open(f'{FEATURE_DIR}/selected_features.txt', 'r') as f:\n        selected_features = f.read().splitlines()\n        if len(selected_features) and selected_features[-1] == '':\n            selected_features = selected_features[:-1]\n\n    #创建一个姊妹模型不考虑滞后特征\n    if not LAGGED:\n        lagged_feats = [\n            'lagged_forward_returns', 'lagged_risk_free_rate', 'lagged_target'\n        ]\n        selected_features = [feat for feat in selected_features if feat not in lagged_feats]\n     \n    return selected_features\n\ndef create_dataset(\n    df,\n    noneFeatureCols=['date_id', 'target','forward_returns','risk_free_rate']) -> pd.DataFrame:\n    \"\"\"\n    Feature engineering with optional noise feature injection.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n        INJECT_NOISE (bool): Whether to inject random noise features. If False, the\n            parameters `noise_features` and `noise_seed` are ignored and may be None.\n        noise_features (Optional[int]): Number of noise features to add if INJECT_NOISE is True.\n        noise_seed (Optional[int]): Random seed for reproducibility when INJECT_NOISE is True.\n\n    Returns:\n        pd.DataFrame: The DataFrame with new features, selected columns, and optional noise features.\n    \"\"\"\n\n    # 用于调用predict API\n    if not isinstance(df, pd.DataFrame):\n        df = df.to_pandas()\n\n    # --- Data Cleaning ---\n    df = data_cleaning(df)\n\n    # --- Feature Engineering ---\n    df = feature_engineering(df)\n    \n    # --- Select Features ---\n    FEATURES = load_selected_features()\n\n    selected_cols = [\"date_id\", \"target\",\"forward_returns\", \"risk_free_rate\"] + FEATURES\n\n    # Ensure selected columns exist in df\n    selected_cols = [c for c in selected_cols if c in df.columns]\n\n    return df.loc[:, selected_cols]\n\n\n\n#%%==================Preparation======================\n\n# Load Models\nmodel_q50_LGBM= load_model(custom_name=f\"LGBM_model_q50\")\nmodels_tail_LGBM=[]\nfor j, a in enumerate(np.arange(TAIL_ALPHA_STEP, TAIL_ALPHA + 1e-8, TAIL_ALPHA_STEP)):\n    model_tail= load_model(custom_name=f\"LGBM_model_tail_{a:.3f}\")\n    models_tail_LGBM.append(model_tail)\nmodel_var_LGBM= load_model(custom_name=f\"LGBM_model_var\")\n\nmodel_q50_CATBOOST=load_model(custom_name=f\"CATBOOST_model_q50\")\nmodels_tail_CATBOOST=[]\nfor j, a in enumerate(np.arange(TAIL_ALPHA_STEP, TAIL_ALPHA + 1e-8, TAIL_ALPHA_STEP)):\n    model_tail= load_model(custom_name=f\"CATBOOST_model_tail_{a:.3f}\")\n    models_tail_CATBOOST.append(model_tail)\nmodel_var_CATBOOST= load_model(custom_name=f\"CATBOOST_model_var\")\n\nmodel_q50_XGBOOST=load_model(custom_name=f\"XGBOOST_model_q50\")\nmodels_tail_XGBOOST=[]\nfor j, a in enumerate(np.arange(TAIL_ALPHA_STEP, TAIL_ALPHA + 1e-8, TAIL_ALPHA_STEP)):\n    model_tail= load_model(custom_name=f\"XGBOOST_model_tail_{a:.3f}\")\n    models_tail_XGBOOST.append(model_tail)\nmodel_var_XGBOOST= load_model(custom_name=f\"XGBOOST_model_var\")\n\n# Create Signal Generators\ngenerator_LGBM=SignalGenerator(\n    model_q50=model_q50_LGBM,\n    models_tail=models_tail_LGBM,\n    model_var=model_var_LGBM,\n    ensemble_weight=STRATEGY_ENSEMBLE_WEIGHT\n)\n\ngenerator_CATBOOST=SignalGenerator(\n    model_q50=model_q50_CATBOOST,\n    models_tail=models_tail_CATBOOST,\n    model_var=model_var_CATBOOST,\n    ensemble_weight=STRATEGY_ENSEMBLE_WEIGHT\n)\n\ngenerator_XGBOOST=SignalGenerator(\n    model_q50=model_q50_XGBOOST,\n    models_tail=models_tail_XGBOOST,\n    model_var=model_var_XGBOOST,\n    ensemble_weight=STRATEGY_ENSEMBLE_WEIGHT\n)\n\n\n#%% =================Prediction API==================\n\ndef predict(test:pl.DataFrame) -> float:\n\n    # Data Pipeline\n    df: pd.DataFrame = create_dataset(test)\n    FEATURES: list[str] = load_selected_features()\n    X_test: pd.DataFrame = df[FEATURES]   # 这里通常只有 1 行\n    \n    # Generate Signal\n    signal_series_LGBM=generator_LGBM.call_ensemble(X_test)\n    signal_series_CATBOOST=generator_CATBOOST.call_ensemble(X_test)\n    signal_series_XGBOOST=generator_XGBOOST.call_ensemble(X_test)\n\n    #signal_series= MODEL_ENSEMBLE_WEIGHT * signal_series_LGBM + (1 - MODEL_ENSEMBLE_WEIGHT) * signal_series_CATBOOST\n    signal_series= (LGBM_ENSEMBLE_WEIGHT * signal_series_LGBM +\n                    CATBOOST_ENSEMBLE_WEIGHT * signal_series_CATBOOST +\n                    XGBOOST_ENSEMBLE_WEIGHT * signal_series_XGBOOST)\n\n    #严格限制在[min_signal, max_signal]范围内\n    signal_series = signal_series.clip(lower=MIN_SIGNAL, upper=MAX_SIGNAL)\n    signal = float(signal_series.iloc[0])\n\n    #保底，防止NaN或Inf被拒绝\n    if not np.isfinite(signal):\n        signal = 1.0 \n\n    return signal\n\ninference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'): #检查环境变量，如果存在则表示在Kaggle竞赛平台上运行\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway((DATA_PATH,))","metadata":{"_uuid":"83113567-d4f8-465a-b73d-40df68d25b7f","_cell_guid":"307df654-c72a-485d-bfc0-528cdf0a88df","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-12-14T17:48:05.744245Z","iopub.execute_input":"2025-12-14T17:48:05.744525Z","iopub.status.idle":"2025-12-14T17:48:14.075756Z","shell.execute_reply.started":"2025-12-14T17:48:05.744503Z","shell.execute_reply":"2025-12-14T17:48:14.07478Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## On End-to-End Learning (Value-Oriented)\n\nEnd-to-end predict-then-optimize learning is definitely trending, but after several rounds of forecasting practice, I’ve come to feel that it’s still very hard to *truly* train a forecasting model directly on **decision value** in settings like this competition.\n\nA major obstacle is that this is essentially a **single-step prediction + decision** problem, while the evaluation is a **window-level score** (Sharpe with non-linear penalties). In such a setting, we usually cannot write the decision value as a clean closed-form objective, nor can we cast it as a convex optimization problem. That means we cannot clearly answer a fundamental question: *what is the actual marginal “value” of improving a one-step forecast by some amount?*\n\nAnother issue is the additional complexity introduced by end-to-end training. Replacing highly efficient ML frameworks (e.g., gradient boosting) with an end-to-end pipeline that embeds a decision model often makes the workflow significantly heavier, while paradoxically making it harder to learn a simple and reasonable representation. As I mentioned in my earlier post, a regression model’s representation should arguably be **as simple as possible**—everything else is basically feature engineering.\n\nThat said, *forecast value* is still a fascinating topic. In equity markets, even when the $R^2$ hovers around zero, the signal can still be economically meaningful and monetizable, which is honestly quite magical. Also, even if we don’t train the forecasting model in a value-oriented way, we can still evaluate **hyperparameters** by decision performance—and this is extremely practical. Hyperparameter search is fundamentally heuristic anyway (trial-based optimization rather than a fully modelable objective), so it makes perfect sense that the target is no longer just predictive accuracy, but rather **decision utility**.\n\nIn summary, my current belief is:\n- **Model parameters** should be trained with efficient frameworks using simple, smooth statistical losses (e.g., MAE/quantile loss), and\n- **Hyperparameters** can be tuned with trial-based methods directly on decision metrics (score/utility),\nbecause the hyperparameter space is much smaller, and the complexity of full end-to-end training is far more manageable at that level than at the parameter level.","metadata":{"_uuid":"c6f29d1c-1af8-43c4-b699-eaf704de7d61","_cell_guid":"4d7231fc-ec53-4409-a164-fe7eea366933","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## On Mindset\n\nOne last thing that feels worth sharing is the mindset required for forecasting practice. With a limited dataset, weak predictability, and a short online evaluation window, this is still a competition heavily driven by randomness. Sometimes it even makes you question time-series forecasting itself: *will what we predict really remain valid in the future?* I don’t think any forecaster can guarantee that.\n\nWhen you finally finish building a solution and see good offline results, you feel happy—but also strangely powerless. Even if you beat the market on average, those folds with ridiculously bad scores can still make you doubt the whole craft and feel discouraged. This is, in the end, a statistical game: no method can make every single sample point “good.” Trying to make everything “all the best” is often just the trap of overfitting. The real question is: *will tomorrow be one of those bad samples?* No one knows.\n\nIndeed, discussion board  is full of comments saying this is a luck-driven competition, and I agree that the final leaderboard can be extremely noisy due to the short test window. However, the only thing we can do is to keep analyzing the data, deriving better theory, refining algorithms, and improving statistical performance. Don’t get overly excited by a lucky sample, and don’t let a bad sample ruin your mental state. Trust the power of statistics—only time will tell. Maybe that’s what “faith in forecasting” means.","metadata":{"_uuid":"113fc7ac-4ab6-4a94-a272-4cd560830afc","_cell_guid":"eec60736-532d-485c-a401-8ded92daf927","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}