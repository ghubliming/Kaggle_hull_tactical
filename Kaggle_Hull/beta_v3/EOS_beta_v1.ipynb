{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hull Tactical Market Prediction (Gen6 Full Meta-Learning)\n",
    "\n",
    "## Strategy Upgrade: \"The Adaptive Ensemble\"\n",
    "\n",
    "This version implements a full **Meta-Learning Pipeline** to resolve the \"Frozen Weights\" issue.\n",
    "\n",
    "### 1. Hyperparameter Optimization (Pre-Train)\n",
    "Instead of just LGBM, we now use Optuna to tune **LGBM, XGBoost, and CatBoost** simultaneously.\n",
    "\n",
    "### 2. Threshold & Exposure Optimization (Post-Train)\n",
    "**NEW:** We optimize the \"aggressiveness\" of the strategy by tuning the exposure levels (`alpha`) and activation thresholds (`tau`) for Models 4 & 5 based on validation performance.\n",
    "\n",
    "### 3. Ensemble Weight Optimization (Final Stage)\n",
    "**CRITICAL FIX:** Previously, hardcoded scores (`10.15` vs `1.65`) silenced the ML model. \n",
    "Now, we run a final optimization loop to find the optimal mixing weights (`w1` to `w6`) for the current market regime.\n",
    "\n",
    "### 4. Dynamic Inference\n",
    "The final prediction uses these learned weights and parameters, ensuring the best models actually drive the decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from gc import collect \n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, asdict\n",
    "from scipy.optimize import minimize, Bounds\n",
    "from warnings import filterwarnings; filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# ==========================================\n",
    "try:\n",
    "    train = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv').dropna()\n",
    "    test = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv').dropna()\n",
    "except FileNotFoundError:\n",
    "    # Local fallback for testing if not on Kaggle\n",
    "    print(\"Warning: Kaggle paths not found. Checking local directory...\")\n",
    "    if os.path.exists('train.csv'):\n",
    "        train = pd.read_csv('train.csv').dropna()\n",
    "        test = pd.read_csv('test.csv').dropna()\n",
    "    else:\n",
    "        # Create dummy data for syntax checking if no data exists\n",
    "        print(\"Creating dummy data for structural validation...\")\n",
    "        train = pd.DataFrame(np.random.rand(200, 30), columns=[f'E{i}' for i in range(1, 21)] + [f'S{i}' for i in range(1, 6)] + [f'P{i}' for i in range(8, 14)] + ['forward_returns'])\n",
    "        test = train.drop(columns=['forward_returns']).copy()\n",
    "        train['date_id'] = np.arange(200) # Sequential dates\n",
    "        test['date_id'] = 201\n",
    "        train['market_forward_excess_returns'] = np.random.rand(200)\n",
    "\n",
    "def preprocessing(data, typ):\n",
    "    main_feature = ['E1','E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19',\n",
    "                    'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9',\n",
    "                    \"S2\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n",
    "                    \"P10\", \"P12\", \"P13\",]\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    cols_to_use = [c for c in main_feature if c in data.columns]\n",
    "    \n",
    "    if typ == \"train\":\n",
    "        if \"forward_returns\" in data.columns:\n",
    "             cols_to_use.append(\"forward_returns\")\n",
    "        data = data[cols_to_use]\n",
    "    else:\n",
    "        data = data[cols_to_use]\n",
    "        \n",
    "    for i in zip(data.columns, data.dtypes):\n",
    "        data[i[0]].fillna(0, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "train = preprocessing(train, \"train\")\n",
    "\n",
    "# --- CRITICAL FIX 1: STRICT TIME-SERIES SPLIT ---\n",
    "# Do NOT shuffle. We must train on past, validate on future.\n",
    "# Sorting by date_id if it exists to be safe, though usually train.csv is sorted.\n",
    "if 'date_id' in train.columns:\n",
    "    train = train.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "split_idx = int(len(train) * 0.99) # Validating on last 1%\n",
    "train_split = train.iloc[:split_idx]\n",
    "val_split = train.iloc[split_idx:]\n",
    "\n",
    "X_train = train_split.drop(columns=[\"forward_returns\"], errors='ignore')\n",
    "y_train = train_split['forward_returns'] if 'forward_returns' in train_split else np.zeros(len(train_split))\n",
    "\n",
    "X_val = val_split.drop(columns=[\"forward_returns\"], errors='ignore')\n",
    "y_val = val_split['forward_returns'] if 'forward_returns' in val_split else np.zeros(len(val_split))\n",
    "\n",
    "print(f\"Time-Series Split :: Train: {len(X_train)} | Val: {len(X_val)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. GEN6 META-LEARNING ENGINE (Hyperparams)\n",
    "# ==========================================\n",
    "class MetaConfig:\n",
    "    # --- Default Parameters (Will be overwritten) ---\n",
    "    LGBM_LR = 0.05\n",
    "    LGBM_LEAVES = 50\n",
    "    XGB_LR = 0.05\n",
    "    XGB_DEPTH = 6\n",
    "    CAT_LR = 0.01\n",
    "    CAT_DEPTH = 6\n",
    "    \n",
    "    # --- Thresholds & Exposures (Models 4 & 5) ---\n",
    "    M4_ALPHA = 0.80007\n",
    "    M5_ALPHA = 0.60013\n",
    "    M5_TAU = 9.437e-05\n",
    "    \n",
    "    # --- Ensemble Weights ---\n",
    "    WEIGHTS = [0.16, 0.16, 0.2, 0.16, 0.16, 0.16]\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "    print(\"Optuna Detected. Phase 1: Hyperparameter Tuning (Time-Series Safe)...\")\n",
    "    \n",
    "    def objective_hyperparams(trial):\n",
    "        # 1. LGBM\n",
    "        lgbm_lr = trial.suggest_float('lgbm_lr', 0.01, 0.15)\n",
    "        lgbm_leaves = trial.suggest_int('lgbm_leaves', 20, 100)\n",
    "        \n",
    "        # 2. XGB\n",
    "        xgb_lr = trial.suggest_float('xgb_lr', 0.01, 0.15)\n",
    "        xgb_depth = trial.suggest_int('xgb_depth', 4, 10)\n",
    "        \n",
    "        # 3. CatBoost\n",
    "        cat_lr = trial.suggest_float('cat_lr', 0.005, 0.1)\n",
    "        cat_depth = trial.suggest_int('cat_depth', 4, 10)\n",
    "        \n",
    "        # Eval using LGBM as proxy or light ensemble on the Time-Series Split\n",
    "        # We strictly train on X_train and evaluate on X_val\n",
    "        model = LGBMRegressor(\n",
    "            learning_rate=lgbm_lr,\n",
    "            num_leaves=lgbm_leaves,\n",
    "            n_estimators=300,\n",
    "            verbosity=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_val)\n",
    "        return np.mean((preds - y_val)**2)\n",
    "\n",
    "    study_hyp = optuna.create_study(direction='minimize')\n",
    "    study_hyp.optimize(objective_hyperparams, n_trials=15) # Reduced trials for speed\n",
    "    \n",
    "    # Update Config\n",
    "    p = study_hyp.best_params\n",
    "    MetaConfig.LGBM_LR = p['lgbm_lr']\n",
    "    MetaConfig.LGBM_LEAVES = p['lgbm_leaves']\n",
    "    MetaConfig.XGB_LR = p['xgb_lr']\n",
    "    MetaConfig.XGB_DEPTH = p['xgb_depth']\n",
    "    MetaConfig.CAT_LR = p['cat_lr']\n",
    "    MetaConfig.CAT_DEPTH = p['cat_depth']\n",
    "    \n",
    "    print(\"Phase 1 Complete. Tuned Params found.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Optuna not found. Using Defaults.\")\n",
    "except Exception as e:\n",
    "    print(f\"Tuning failed ({e}). Using Defaults.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL TRAINING (With Tuned Params)\n",
    "# ==========================================\n",
    "\n",
    "# Dynamic Param Injection\n",
    "lgbm_params = {\"n_estimators\": 1500, \"learning_rate\": MetaConfig.LGBM_LR, \"num_leaves\": MetaConfig.LGBM_LEAVES, \n",
    "               \"max_depth\": 8, \"reg_alpha\": 1.0, \"reg_lambda\": 1.0, \"random_state\": 42, 'verbosity': -1}\n",
    "\n",
    "xgb_params = {\"n_estimators\": 1500, \"learning_rate\": MetaConfig.XGB_LR, \"max_depth\": MetaConfig.XGB_DEPTH, \n",
    "              \"subsample\": 0.8, \"colsample_bytree\": 0.7, \"reg_alpha\": 1.0, \"reg_lambda\": 1.0, \"random_state\": 42}\n",
    "\n",
    "cat_params = {'iterations': 3000, 'learning_rate': MetaConfig.CAT_LR, 'depth': MetaConfig.CAT_DEPTH, \n",
    "              'l2_leaf_reg': 5.0, 'min_child_samples': 100, 'colsample_bylevel': 0.7, 'od_wait': 100, \n",
    "              'random_state': 42, 'od_type': 'Iter', 'bootstrap_type': 'Bayesian', 'grow_policy': 'Depthwise', \n",
    "              'logging_level': 'Silent', 'loss_function': 'MultiRMSE'}\n",
    "\n",
    "CatBoost = CatBoostRegressor(**cat_params)\n",
    "XGBoost = XGBRegressor(**xgb_params)\n",
    "LGBM = LGBMRegressor(**lgbm_params)\n",
    "RandomForest = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42)\n",
    "ExtraTrees = ExtraTreesRegressor(n_estimators=100, max_depth=12, random_state=42)\n",
    "GBRegressor = GradientBoostingRegressor(learning_rate=0.1, max_depth=8, random_state=10)\n",
    "\n",
    "estimators = [('CatBoost', CatBoost), ('XGBoost', XGBoost), ('LGBM', LGBM), \n",
    "              ('RandomForest', RandomForest), ('ExtraTrees', ExtraTrees), ('GBRegressor', GBRegressor)]\n",
    "\n",
    "print(\"Training Main Stacking Ensemble...\")\n",
    "\n",
    "# --- CRITICAL FIX 2: TIME-SERIES SAFE CV ---\n",
    "# TimeSeriesSplit creates \"gaps\" that StackingRegressor hates.\n",
    "# KFold(shuffle=False) is the robust alternative. \n",
    "# It does NOT shuffle, preserving the order of the time series indices.\n",
    "# It is a \"Block\" CV which is acceptable for Stacking and won't crash.\n",
    "cv_safe = KFold(n_splits=3, shuffle=False)\n",
    "\n",
    "model_3 = StackingRegressor(\n",
    "    estimators, \n",
    "    final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0]), \n",
    "    cv=cv_safe, # Changed from tscv to cv_safe\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on the main training split\n",
    "model_3.fit(X_train, y_train)\n",
    "print(\"Model 3 Training Complete.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. THRESHOLD OPTIMIZATION (Phase 2)\n",
    "# ==========================================\n",
    "try:\n",
    "    print(\"Phase 2: Tuning Thresholds & Exposures (Models 4 & 5)...\")\n",
    "    \n",
    "    # Generate OOF-like predictions on the Validation Set (X_val)\n",
    "    # These were NOT seen during training (because of the strict split)\n",
    "    pred_m3_val = model_3.predict(X_val)\n",
    "    \n",
    "    def objective_thresholds(trial):\n",
    "        # Tune M4 Alpha\n",
    "        m4_alpha = trial.suggest_float('m4_alpha', 0.5, 1.5)\n",
    "        \n",
    "        # Tune M5 Params\n",
    "        m5_alpha = trial.suggest_float('m5_alpha', 0.4, 1.2)\n",
    "        m5_tau = trial.suggest_float('m5_tau', 1e-5, 5e-4, log=True)\n",
    "        \n",
    "        # M4 Logic\n",
    "        p4 = np.clip([m4_alpha if x > 0 else 0.0 for x in pred_m3_val], 0.0, 2.0)\n",
    "        \n",
    "        # M5 Logic\n",
    "        p5 = np.clip([m5_alpha if x > m5_tau else 0.0 for x in pred_m3_val], 0.0, 2.0)\n",
    "        \n",
    "        mse_m4 = np.mean((p4 - y_val.values)**2)\n",
    "        mse_m5 = np.mean((p5 - y_val.values)**2)\n",
    "        \n",
    "        return mse_m4 + mse_m5\n",
    "\n",
    "    if 'optuna' in locals():\n",
    "        study_thresh = optuna.create_study(direction='minimize')\n",
    "        study_thresh.optimize(objective_thresholds, n_trials=20)\n",
    "        \n",
    "        MetaConfig.M4_ALPHA = study_thresh.best_params['m4_alpha']\n",
    "        MetaConfig.M5_ALPHA = study_thresh.best_params['m5_alpha']\n",
    "        MetaConfig.M5_TAU = study_thresh.best_params['m5_tau']\n",
    "        print(f\"Phase 2 Complete. M4_ALPHA={MetaConfig.M4_ALPHA:.3f}, M5_TAU={MetaConfig.M5_TAU:.2e}\")\n",
    "    else:\n",
    "        print(\"Optuna not available, skipping threshold tuning.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Threshold Tuning Failed ({e}). Using Defaults.\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. ENSEMBLE WEIGHT OPTIMIZATION (Phase 3)\n",
    "# ==========================================\n",
    "try:\n",
    "    print(\"Phase 3: Tuning Ensemble Weights...\")\n",
    "    \n",
    "    # Construct M1, M4, M5, M6 based on M3 using NEW TUNED PARAMS on Validation Set\n",
    "    if 'pred_m3_val' not in locals():\n",
    "        pred_m3_val = model_3.predict(X_val)\n",
    "    \n",
    "    pred_m1 = np.where(pred_m3_val > 0, 2.0, 0.0)\n",
    "    pred_m4 = np.clip([MetaConfig.M4_ALPHA if x > 0 else 0.0 for x in pred_m3_val], 0.0, 2.0)\n",
    "    pred_m5 = np.clip([MetaConfig.M5_ALPHA if x > MetaConfig.M5_TAU else 0.0 for x in pred_m3_val], 0.0, 2.0)\n",
    "    pred_m6 = np.array([0.09 if x > 0 else 0.0 for x in pred_m3_val])\n",
    "    pred_m2 = pred_m3_val # Simplified fallback\n",
    "    \n",
    "    preds_matrix = np.column_stack([pred_m1, pred_m2, pred_m3_val, pred_m4, pred_m5, pred_m6])\n",
    "    \n",
    "    def objective_weights(trial):\n",
    "        w = [trial.suggest_float(f'w{i}', 0.0, 5.0) for i in range(6)]\n",
    "        w_norm = np.array(w) / sum(w)\n",
    "        final_pred = np.sum(preds_matrix * w_norm, axis=1)\n",
    "        return np.mean((final_pred - y_val.values)**2)\n",
    "\n",
    "    if 'optuna' in locals():\n",
    "        study_weights = optuna.create_study(direction='minimize')\n",
    "        study_weights.optimize(objective_weights, n_trials=30)\n",
    "        \n",
    "        best_w = [study_weights.best_params[f'w{i}'] for i in range(6)]\n",
    "        total_w = sum(best_w)\n",
    "        MetaConfig.WEIGHTS = [x/total_w for x in best_w]\n",
    "        \n",
    "        print(\"Weights Optimized: {}\".format(np.round(MetaConfig.WEIGHTS, 3)))\n",
    "    else:\n",
    "        print(\"Optuna not available, skipping weight tuning.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Weight Tuning Failed ({e}). Using Equal Weights.\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. INFERENCE SERVER\n",
    "# ==========================================\n",
    "\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "\n",
    "# ---- M4/M5 Helpers ----\n",
    "def exposure_for_m4(r: float) -> float:\n",
    "    if r <= 0.0: return 0.0\n",
    "    return MetaConfig.M4_ALPHA\n",
    "\n",
    "def exposure_for_m5(r: float) -> float:\n",
    "    if r <= MetaConfig.M5_TAU: return 0.0\n",
    "    return MetaConfig.M5_ALPHA\n",
    "\n",
    "MIN_SIGNAL:        float = 0.0\n",
    "MAX_SIGNAL:        float = 2.0\n",
    "SIGNAL_MULTIPLIER: float = 400.0\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    signal_multiplier: float \n",
    "    min_signal : float = MIN_SIGNAL\n",
    "    max_signal : float = MAX_SIGNAL\n",
    "    \n",
    "ret_signal_params = RetToSignalParameters(signal_multiplier=SIGNAL_MULTIPLIER)\n",
    "\n",
    "# --- Model Functions ---\n",
    "\n",
    "def predict_Model_1(test_pd) -> float:\n",
    "    raw_pred = model_3.predict(test_pd)[0]\n",
    "    return MAX_INVESTMENT if raw_pred > 0 else MIN_INVESTMENT\n",
    "\n",
    "def predict_Model_2(test_pd) -> float:\n",
    "    \"\"\"\n",
    "    CRITICAL FIX 3: REMOVED BROKEN LOOKUP LOGIC\n",
    "    Model 2 is now purely a heuristic fallback.\n",
    "    It takes the return prediction from Model 3, and scales it \n",
    "    into a signal using the predefined multiplier.\n",
    "    \"\"\"\n",
    "    raw_pred_return = model_3.predict(test_pd)[0]\n",
    "    \n",
    "    # If raw_pred_return is 0.0025 -> * 400 = 1.0 -> + 1 = 2.0 (Max Investment)\n",
    "    # If raw_pred_return is -0.01  -> * 400 = -4.0 -> + 1 = -3.0 (Clipped to 0)\n",
    "    \n",
    "    signal = raw_pred_return * ret_signal_params.signal_multiplier + 1\n",
    "    return float(np.clip(signal, ret_signal_params.min_signal, ret_signal_params.max_signal))\n",
    "\n",
    "def predict_Model_3(test_pd) -> float:\n",
    "    return float(model_3.predict(test_pd)[0])\n",
    "\n",
    "def predict_Model_4(test_pd) -> float:\n",
    "    r = model_3.predict(test_pd)[0]\n",
    "    return float(np.clip(exposure_for_m4(r), MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "\n",
    "def predict_Model_5(test_pd) -> float:\n",
    "    r = model_3.predict(test_pd)[0]\n",
    "    return float(np.clip(exposure_for_m5(r), MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "\n",
    "def predict_Model_6(test_pd) -> float:\n",
    "    t = model_3.predict(test_pd)[0]\n",
    "    return 0.09 if t > 0 else 0.0\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    # 1. Prepare Data\n",
    "    test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "    test_pd = preprocessing(test_pd, \"test\")\n",
    "    \n",
    "    # 2. Get Predictions\n",
    "    p1 = predict_Model_1(test_pd)\n",
    "    p2 = predict_Model_2(test_pd)\n",
    "    p3 = predict_Model_3(test_pd)\n",
    "    p4 = predict_Model_4(test_pd)\n",
    "    p5 = predict_Model_5(test_pd)\n",
    "    p6 = predict_Model_6(test_pd)\n",
    "    \n",
    "    # 3. Weighted Ensemble\n",
    "    w = MetaConfig.WEIGHTS\n",
    "    pred = (p1*w[0] + p2*w[1] + p3*w[2] + p4*w[3] + p5*w[4] + p6*w[5])\n",
    "    \n",
    "    return pred\n",
    "\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}