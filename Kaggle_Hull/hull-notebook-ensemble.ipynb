{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94632cec",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from typing import List\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import polars as pl \n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import kaggle_evaluation.default_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ada02",
   "metadata": {},
   "source": [
    "## Project Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc03c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fc31f",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dad2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ PATHS ============\n",
    "DATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
    "\n",
    "# ============ RETURNS TO SIGNAL CONFIGS ============\n",
    "MIN_SIGNAL: float = 0.0                         # Minimum value for the daily signal \n",
    "MAX_SIGNAL: float = 2.0                         # Maximum value for the daily signal \n",
    "SIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal \n",
    "\n",
    "# ============ MODEL CONFIGS ============\n",
    "N_SPLITS: int = 5                               # Number of time series cross validation splits\n",
    "RIDGE_ALPHAS: np.ndarray = np.logspace(-3, 3, 50)  # Ridge regularization parameters to test\n",
    "LASSO_ALPHAS: np.ndarray = np.logspace(-4, 1, 50)  # Lasso regularization parameters to test\n",
    "USE_ROBUST_SCALER: bool = True                  # Use RobustScaler (better for outliers) vs StandardScaler\n",
    "ENSEMBLE_WEIGHTS: dict = {'ridge': 0.5, 'lasso': 0.3, 'ols': 0.2}  # Weights for ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5126c60",
   "metadata": {},
   "source": [
    "## Dataclasses Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetOutput:\n",
    "    X_train : pl.DataFrame \n",
    "    X_test: pl.DataFrame\n",
    "    y_train: pl.Series\n",
    "    y_test: pl.Series\n",
    "    scaler: StandardScaler | RobustScaler\n",
    "\n",
    "@dataclass \n",
    "class ModelParameters:\n",
    "    n_splits: int\n",
    "    ridge_alphas: np.ndarray\n",
    "    lasso_alphas: np.ndarray\n",
    "    use_robust_scaler: bool\n",
    "    ensemble_weights: dict\n",
    "    \n",
    "    def __post_init__(self): \n",
    "        if self.n_splits < 2:\n",
    "            raise ValueError(\"n_splits must be at least 2 for TimeSeriesSplit\")\n",
    "        if sum(self.ensemble_weights.values()) != 1.0:\n",
    "            raise ValueError(\"Ensemble weights must sum to 1.0\")\n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    signal_multiplier: float \n",
    "    min_signal : float = MIN_SIGNAL\n",
    "    max_signal : float = MAX_SIGNAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89ae86",
   "metadata": {},
   "source": [
    "## Set the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_signal_params = RetToSignalParameters(\n",
    "    signal_multiplier= SIGNAL_MULTIPLIER\n",
    ")\n",
    "\n",
    "model_params = ModelParameters(\n",
    "    n_splits=N_SPLITS,\n",
    "    ridge_alphas=RIDGE_ALPHAS,\n",
    "    lasso_alphas=LASSO_ALPHAS,\n",
    "    use_robust_scaler=USE_ROBUST_SCALER,\n",
    "    ensemble_weights=ENSEMBLE_WEIGHTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837b7c2",
   "metadata": {},
   "source": [
    "## Dataset Loading/Creating Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2251f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainset() -> pl.DataFrame:\n",
    "    \"\"\"Loads and preprocesses the training dataset.\"\"\" \n",
    "    return (\n",
    "        pl.read_csv(DATA_PATH / \"train.csv\")\n",
    "        .rename({'market_forward_excess_returns':'target'})\n",
    "        .with_columns(\n",
    "            pl.exclude('date_id').cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .head(-10)\n",
    "    )\n",
    "\n",
    "def load_testset() -> pl.DataFrame:\n",
    "    \"\"\"Loads and preprocesses the testing dataset.\"\"\" \n",
    "    return (\n",
    "        pl.read_csv(DATA_PATH / \"test.csv\")\n",
    "        .rename({'lagged_forward_returns':'target'})\n",
    "        .with_columns(\n",
    "            pl.exclude('date_id').cast(pl.Float64, strict=False)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Creates new features and cleans a DataFrame.\"\"\" \n",
    "    vars_to_keep: List[str] = [\n",
    "        \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n",
    "        \"P10\", \"P12\", \"P13\", \"U1\", \"U2\", \"U3\", \"U4\", \"U5\"\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        df.with_columns([\n",
    "            (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
    "            (pl.col(\"M11\") / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)).alias(\"U2\"),\n",
    "            (pl.col(\"S2\") / (pl.col(\"S1\") + 1e-6)).alias(\"U3\"),\n",
    "            (pl.col(\"E2\") * pl.col(\"E3\")).alias(\"U4\"),\n",
    "            (pl.col(\"P9\") + pl.col(\"P10\") + pl.col(\"P12\")).alias(\"U5\"),\n",
    "        ])\n",
    "        .select([\"date_id\", \"target\"] + vars_to_keep)\n",
    "        .with_columns([\n",
    "            pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n",
    "            for col in vars_to_keep\n",
    "        ])\n",
    "        .drop_nulls()\n",
    "    )\n",
    "    \n",
    "def join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Joins two dataframes by common columns and concatenates them vertically.\"\"\" \n",
    "    common_columns: list[str] = [col for col in train.columns if col in test.columns]\n",
    "    return pl.concat([train.select(common_columns), test.select(common_columns)], how=\"vertical\")\n",
    "\n",
    "def split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str], use_robust: bool = True) -> DatasetOutput: \n",
    "    \"\"\"Splits the data into features (X) and target (y), and scales the features.\"\"\" \n",
    "    X_train = train.drop(['date_id','target']) \n",
    "    y_train = train.get_column('target')\n",
    "    X_test = test.drop(['date_id','target']) \n",
    "    y_test = test.get_column('target')\n",
    "    \n",
    "    scaler = RobustScaler() if use_robust else StandardScaler()\n",
    "    \n",
    "    X_train_scaled_np = scaler.fit_transform(X_train)\n",
    "    X_train = pl.from_numpy(X_train_scaled_np, schema=features)\n",
    "    \n",
    "    X_test_scaled_np = scaler.transform(X_test)\n",
    "    X_test = pl.from_numpy(X_test_scaled_np, schema=features)\n",
    "    \n",
    "    return DatasetOutput(\n",
    "        X_train = X_train,\n",
    "        y_train = y_train, \n",
    "        X_test = X_test, \n",
    "        y_test = y_test,\n",
    "        scaler = scaler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02cfb5",
   "metadata": {},
   "source": [
    "## Converting Return Prediction to Signal\\n\\nHere is an example of a potential function used to convert a prediction based on the market forward excess return to a daily signal position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ecc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ret_to_signal(\n",
    "    ret_arr: np.ndarray,\n",
    "    params: RetToSignalParameters\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Converts raw model predictions (expected returns) into a trading signal.\"\"\" \n",
    "    return np.clip(\n",
    "        ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707dbf6",
   "metadata": {},
   "source": [
    "## Looking at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train: pl.DataFrame = load_trainset()\n",
    "test: pl.DataFrame = load_testset() \n",
    "print(train.tail(3)) \n",
    "print(test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2ae9b",
   "metadata": {},
   "source": [
    "## Generating the Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pl.DataFrame = join_train_test_dataframes(train, test)\n",
    "df = create_example_dataset(df=df) \n",
    "train: pl.DataFrame = df.filter(pl.col('date_id').is_in(train.get_column('date_id')))\n",
    "test: pl.DataFrame = df.filter(pl.col('date_id').is_in(test.get_column('date_id')))\n",
    "\n",
    "FEATURES: list[str] = [col for col in test.columns if col not in ['date_id', 'target']]\n",
    "\n",
    "dataset: DatasetOutput = split_dataset(train=train, test=test, features=FEATURES, use_robust=model_params.use_robust_scaler)\n",
    "\n",
    "X_train: pl.DataFrame = dataset.X_train\n",
    "X_test: pl.DataFrame = dataset.X_test\n",
    "y_train: pl.DataFrame = dataset.y_train\n",
    "y_test: pl.DataFrame = dataset.y_test\n",
    "scaler: StandardScaler | RobustScaler = dataset.scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbbb7e8",
   "metadata": {},
   "source": [
    "## Fitting the Model with Ensemble + Time-Series CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def find_best_alpha_timeseries(X, y, alphas: np.ndarray, model_class, n_splits: int = 5) -> Tuple[float, list]:\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    alpha_scores = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        fold_scores = []\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            model = model_class(alpha=alpha)\n",
    "            model.fit(X_tr, y_tr)\n",
    "            pred = model.predict(X_val)\n",
    "            mse = mean_squared_error(y_val, pred)\n",
    "            fold_scores.append(mse)\n",
    "        alpha_scores.append(np.mean(fold_scores))\n",
    "    \n",
    "    best_idx = np.argmin(alpha_scores)\n",
    "    return alphas[best_idx], alpha_scores\n",
    "\n",
    "# Convert to numpy for sklearn\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "\n",
    "# Find best Ridge alpha\n",
    "best_ridge_alpha, _ = find_best_alpha_timeseries(X_train_np, y_train_np, model_params.ridge_alphas, Ridge, model_params.n_splits)\n",
    "# Find best Lasso alpha  \n",
    "best_lasso_alpha, _ = find_best_alpha_timeseries(X_train_np, y_train_np, model_params.lasso_alphas, Lasso, model_params.n_splits)\n",
    "\n",
    "# Create ensemble model\n",
    "model = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('ridge', Ridge(alpha=best_ridge_alpha)),\n",
    "        ('lasso', Lasso(alpha=best_lasso_alpha)),\n",
    "        ('ols', LinearRegression())\n",
    "    ],\n",
    "    weights=[model_params.ensemble_weights['ridge'], model_params.ensemble_weights['lasso'], model_params.ensemble_weights['ols']]\n",
    ")\n",
    "model.fit(X_train_np, y_train_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef03f88",
   "metadata": {},
   "source": [
    "## Prediction Function via Kaggle Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame) -> float:\n",
    "    test = test.rename({'lagged_forward_returns':'target'})\n",
    "    df: pl.DataFrame = create_example_dataset(test)\n",
    "    X_test: pl.DataFrame = df.select(FEATURES)\n",
    "    X_test_scaled_np: np.ndarray = scaler.transform(X_test)\n",
    "    X_test: pl.DataFrame = pl.from_numpy(X_test_scaled_np, schema=FEATURES)\n",
    "    raw_pred: float = model.predict(X_test)[0]\n",
    "    return convert_ret_to_signal(np.array([raw_pred]), ret_signal_params)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886eebb",
   "metadata": {},
   "source": [
    "## Launch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ec6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
