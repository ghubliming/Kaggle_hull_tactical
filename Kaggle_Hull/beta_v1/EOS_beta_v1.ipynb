{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hull Tactical Market Prediction (Gen6 Full Meta-Learning)\n",
    "\n",
    "## Strategy Upgrade: \"The Adaptive Ensemble\"\n",
    "\n",
    "This version implements a full **Meta-Learning Pipeline** to resolve the \"Frozen Weights\" issue.\n",
    "\n",
    "### 1. Hyperparameter Optimization (Pre-Train)\n",
    "Instead of just LGBM, we now use Optuna to tune **LGBM, XGBoost, and CatBoost** simultaneously.\n",
    "\n",
    "### 2. Ensemble Weight Optimization (Post-Train)\n",
    "**CRITICAL FIX:** Previously, hardcoded scores (`10.15` vs `1.65`) silenced the ML model. \n",
    "Now, we run a second optimization loop *after* training to find the optimal mixing weights (`w1` to `w6`) for the current market regime.\n",
    "\n",
    "### 3. Dynamic Inference\n",
    "The final prediction uses these learned weights, ensuring the best models actually drive the decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from gc import collect \n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, asdict\n",
    "from scipy.optimize import minimize, Bounds\n",
    "from warnings import filterwarnings; filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import kaggle_evaluation.default_inference_server\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA LOADING & PREPROCESSING\n",
    "# ==========================================\n",
    "try:\n",
    "    train = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv').dropna()\n",
    "    test = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv').dropna()\n",
    "except FileNotFoundError:\n",
    "    # Local fallback for testing if not on Kaggle\n",
    "    print(\"Warning: Kaggle paths not found. Checking local directory...\")\n",
    "    if os.path.exists('train.csv'):\n",
    "        train = pd.read_csv('train.csv').dropna()\n",
    "        test = pd.read_csv('test.csv').dropna()\n",
    "    else:\n",
    "        # Create dummy data for syntax checking if no data exists\n",
    "        print(\"Creating dummy data for structural validation...\")\n",
    "        train = pd.DataFrame(np.random.rand(100, 30), columns=[f'E{i}' for i in range(1, 21)] + [f'S{i}' for i in range(1, 6)] + [f'P{i}' for i in range(8, 14)] + ['forward_returns'])\n",
    "        test = train.drop(columns=['forward_returns']).copy()\n",
    "        train['date_id'] = 1\n",
    "        test['date_id'] = 2\n",
    "        train['market_forward_excess_returns'] = np.random.rand(100)\n",
    "\n",
    "def preprocessing(data, typ):\n",
    "    main_feature = ['E1','E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19',\n",
    "                    'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9',\n",
    "                    \"S2\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n",
    "                    \"P10\", \"P12\", \"P13\",]\n",
    "    \n",
    "    # Filter to only existing columns to prevent KeyErrors if dummy data is used\n",
    "    cols_to_use = [c for c in main_feature if c in data.columns]\n",
    "    \n",
    "    if typ == \"train\":\n",
    "        if \"forward_returns\" in data.columns:\n",
    "             cols_to_use.append(\"forward_returns\")\n",
    "        data = data[cols_to_use]\n",
    "    else:\n",
    "        data = data[cols_to_use]\n",
    "        \n",
    "    for i in zip(data.columns, data.dtypes):\n",
    "        data[i[0]].fillna(0, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "train = preprocessing(train, \"train\")\n",
    "train_split, val_split = train_test_split(\n",
    "    train, test_size=0.01, random_state=42\n",
    ")\n",
    "\n",
    "X_train = train_split.drop(columns=[\"forward_returns\"], errors='ignore')\n",
    "X_test = val_split.drop(columns=[\"forward_returns\"], errors='ignore')\n",
    "y_train = train_split['forward_returns'] if 'forward_returns' in train_split else np.zeros(len(train_split))\n",
    "y_test = val_split['forward_returns'] if 'forward_returns' in val_split else np.zeros(len(val_split))\n",
    "\n",
    "# ==========================================\n",
    "# 2. GEN6 META-LEARNING ENGINE (Hyperparams)\n",
    "# ==========================================\n",
    "class MetaConfig:\n",
    "    # --- Default Parameters (Will be overwritten) ---\n",
    "    LGBM_LR = 0.05\n",
    "    LGBM_LEAVES = 50\n",
    "    XGB_LR = 0.05\n",
    "    XGB_DEPTH = 6\n",
    "    CAT_LR = 0.01\n",
    "    CAT_DEPTH = 6\n",
    "    \n",
    "    # --- Ensemble Weights (The Fix for Silent Model 3) ---\n",
    "    # Default to equal weights initially\n",
    "    WEIGHTS = [0.16, 0.16, 0.2, 0.16, 0.16, 0.16]\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "    print(\"Optuna Detected. Phase 1: Hyperparameter Tuning...\")\n",
    "    \n",
    "    # --- Subsampling for Speed ---\n",
    "    subset_idx = int(len(X_train) * 0.8)\n",
    "    X_meta = X_train.iloc[subset_idx:]\n",
    "    y_meta = y_train.iloc[subset_idx:]\n",
    "    \n",
    "    def objective_hyperparams(trial):\n",
    "        # Tune ALL 3 Boosters simultaneously for interaction effects\n",
    "        \n",
    "        # 1. LGBM\n",
    "        lgbm_lr = trial.suggest_float('lgbm_lr', 0.01, 0.15)\n",
    "        lgbm_leaves = trial.suggest_int('lgbm_leaves', 20, 100)\n",
    "        \n",
    "        # 2. XGB\n",
    "        xgb_lr = trial.suggest_float('xgb_lr', 0.01, 0.15)\n",
    "        xgb_depth = trial.suggest_int('xgb_depth', 4, 10)\n",
    "        \n",
    "        # 3. CatBoost\n",
    "        cat_lr = trial.suggest_float('cat_lr', 0.005, 0.1)\n",
    "        cat_depth = trial.suggest_int('cat_depth', 4, 10)\n",
    "        \n",
    "        # Quick Evaluation (Using just LGBM as proxy for speed, or light ensemble)\n",
    "        model = LGBMRegressor(\n",
    "            learning_rate=lgbm_lr,\n",
    "            num_leaves=lgbm_leaves,\n",
    "            n_estimators=300,\n",
    "            verbosity=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(X_meta, y_meta, test_size=0.2, shuffle=False)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        return np.mean((preds - y_val)**2)\n",
    "\n",
    "    study_hyp = optuna.create_study(direction='minimize')\n",
    "    study_hyp.optimize(objective_hyperparams, n_trials=20)\n",
    "    \n",
    "    # Update Config\n",
    "    p = study_hyp.best_params\n",
    "    MetaConfig.LGBM_LR = p['lgbm_lr']\n",
    "    MetaConfig.LGBM_LEAVES = p['lgbm_leaves']\n",
    "    MetaConfig.XGB_LR = p['xgb_lr']\n",
    "    MetaConfig.XGB_DEPTH = p['xgb_depth']\n",
    "    MetaConfig.CAT_LR = p['cat_lr']\n",
    "    MetaConfig.CAT_DEPTH = p['cat_depth']\n",
    "    \n",
    "    print(\"Phase 1 Complete. Tuned Params: LGBM_LR={:.3f}, XGB_LR={:.3f}\".format(MetaConfig.LGBM_LR, MetaConfig.XGB_LR))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Optuna not found. Using Defaults.\")\n",
    "except Exception as e:\n",
    "    print(f\"Tuning failed ({e}). Using Defaults.\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. MODEL TRAINING (With Tuned Params)\n",
    "# ==========================================\n",
    "\n",
    "# Dynamic Param Injection\n",
    "lgbm_params = {\"n_estimators\": 1500, \"learning_rate\": MetaConfig.LGBM_LR, \"num_leaves\": MetaConfig.LGBM_LEAVES, \n",
    "               \"max_depth\": 8, \"reg_alpha\": 1.0, \"reg_lambda\": 1.0, \"random_state\": 42, 'verbosity': -1}\n",
    "\n",
    "xgb_params = {\"n_estimators\": 1500, \"learning_rate\": MetaConfig.XGB_LR, \"max_depth\": MetaConfig.XGB_DEPTH, \n",
    "              \"subsample\": 0.8, \"colsample_bytree\": 0.7, \"reg_alpha\": 1.0, \"reg_lambda\": 1.0, \"random_state\": 42}\n",
    "\n",
    "cat_params = {'iterations': 3000, 'learning_rate': MetaConfig.CAT_LR, 'depth': MetaConfig.CAT_DEPTH, \n",
    "              'l2_leaf_reg': 5.0, 'min_child_samples': 100, 'colsample_bylevel': 0.7, 'od_wait': 100, \n",
    "              'random_state': 42, 'od_type': 'Iter', 'bootstrap_type': 'Bayesian', 'grow_policy': 'Depthwise', \n",
    "              'logging_level': 'Silent', 'loss_function': 'MultiRMSE'}\n",
    "\n",
    "CatBoost = CatBoostRegressor(**cat_params)\n",
    "XGBoost = XGBRegressor(**xgb_params)\n",
    "LGBM = LGBMRegressor(**lgbm_params)\n",
    "RandomForest = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42)\n",
    "ExtraTrees = ExtraTreesRegressor(n_estimators=100, max_depth=12, random_state=42)\n",
    "GBRegressor = GradientBoostingRegressor(learning_rate=0.1, max_depth=8, random_state=10)\n",
    "\n",
    "estimators = [('CatBoost', CatBoost), ('XGBoost', XGBoost), ('LGBM', LGBM), \n",
    "              ('RandomForest', RandomForest), ('ExtraTrees', ExtraTrees), ('GBRegressor', GBRegressor)]\n",
    "\n",
    "print(\"Training Main Stacking Ensemble...\")\n",
    "model_3 = StackingRegressor(estimators, final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0]), cv=3)\n",
    "model_3.fit(X_train, y_train)\n",
    "print(\"Model 3 Training Complete.\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. ENSEMBLE WEIGHT OPTIMIZATION (The Fix)\n",
    "# ==========================================\n",
    "try:\n",
    "    print(\"Phase 2: Tuning Ensemble Weights (Unlocking Model 3)...\")\n",
    "    \n",
    "    # Generate Predictions on Hold-out Set (Validation)\n",
    "    # Note: We use X_test/y_test defined earlier\n",
    "    \n",
    "    # Raw Model 3 Preds\n",
    "    pred_m3 = model_3.predict(X_test)\n",
    "    \n",
    "    # Construct M1, M4, M5, M6 based on M3 (Heuristics)\n",
    "    # This mirrors the logic in the predict() functions\n",
    "    pred_m1 = np.where(pred_m3 > 0, 2.0, 0.0)\n",
    "    pred_m4 = np.clip([0.8 if x > 0 else 0.0 for x in pred_m3], 0.0, 2.0)\n",
    "    pred_m5 = np.clip([0.6 if x > 9.43e-5 else 0.0 for x in pred_m3], 0.0, 2.0)\n",
    "    pred_m6 = np.array([0.09 if x > 0 else 0.0 for x in pred_m3])\n",
    "    \n",
    "    # M2 Fallback (Since we might lack external signal in validation)\n",
    "    pred_m2 = pred_m3 # Simplified fallback for weight tuning\n",
    "    \n",
    "    preds_matrix = np.column_stack([pred_m1, pred_m2, pred_m3, pred_m4, pred_m5, pred_m6])\n",
    "    \n",
    "    def objective_weights(trial):\n",
    "        # Suggest raw weights (0 to 10)\n",
    "        w = [trial.suggest_float(f'w{i}', 0.0, 5.0) for i in range(6)]\n",
    "        \n",
    "        # Normalize\n",
    "        w_norm = np.array(w) / sum(w)\n",
    "        \n",
    "        # Weighted Average\n",
    "        final_pred = np.sum(preds_matrix * w_norm, axis=1)\n",
    "        \n",
    "        # Minimize MSE\n",
    "        return np.mean((final_pred - y_test.values)**2)\n",
    "\n",
    "    if 'optuna' in locals():\n",
    "        study_weights = optuna.create_study(direction='minimize')\n",
    "        study_weights.optimize(objective_weights, n_trials=30)\n",
    "        \n",
    "        best_w = [study_weights.best_params[f'w{i}'] for i in range(6)]\n",
    "        total_w = sum(best_w)\n",
    "        MetaConfig.WEIGHTS = [x/total_w for x in best_w]\n",
    "        \n",
    "        print(\"Weights Optimized: {}\".format(np.round(MetaConfig.WEIGHTS, 3)))\n",
    "    else:\n",
    "        print(\"Optuna not available, skipping weight tuning.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Weight Tuning Failed ({e}). Using Equal Weights.\")\n",
    "\n",
    "MIN_INVESTMENT = 0.0\n",
    "MAX_INVESTMENT = 2.0\n",
    "\n",
    "# ---- Fixed best parameter from optimization ----\n",
    "ALPHA_BEST_m4 = 0.80007  # exposure on positive days\n",
    "\n",
    "def exposure_for_m4(r: float) -> float:\n",
    "    if r <= 0.0:\n",
    "        return 0.0\n",
    "    return ALPHA_BEST_m4\n",
    "\n",
    "# ---- Best parameters from Optuna ----\n",
    "ALPHA_BEST_m5 = 0.6001322487531852\n",
    "USE_EXCESS_m5 = False\n",
    "TAU_ABS_m5    = 9.437170708744412e-05  # \u2248 0.01%\n",
    "\n",
    "def exposure_for_m5(r: float, rf: float = 0.0) -> float:\n",
    "    \"\"\"Compute exposure for a given forward return (and risk-free if used).\"\"\"\n",
    "    signal = (r - rf) if USE_EXCESS_m5 else r\n",
    "    if signal <= TAU_ABS_m5:\n",
    "        return 0.0\n",
    "    return ALPHA_BEST_m5\n",
    "\n",
    "MIN_SIGNAL:        float = 0.0                  # Minimum value for the daily signal \n",
    "MAX_SIGNAL:        float = 2.0                  # Maximum value for the daily signal \n",
    "SIGNAL_MULTIPLIER: float = 400.0                # Multiplier of the OLS market forward excess returns predictions to signal \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    signal_multiplier: float \n",
    "    min_signal : float = MIN_SIGNAL\n",
    "    max_signal : float = MAX_SIGNAL\n",
    "    \n",
    "ret_signal_params = RetToSignalParameters ( signal_multiplier= SIGNAL_MULTIPLIER )\n",
    "\n",
    "def predict_Model_1(test: pl.DataFrame) -> float:\n",
    "    # print('Model_1')\n",
    "    # Use Model_3 prediction as base signal\n",
    "    test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "    test_pd = preprocessing(test_pd, \"test\")\n",
    "    raw_pred = model_3.predict(test_pd)[0]\n",
    "    # Binary strategy: full investment if positive prediction\n",
    "    pred_1 = MAX_INVESTMENT if raw_pred > 0 else MIN_INVESTMENT\n",
    "    # print(f'{pred_1}')\n",
    "    return pred_1\n",
    "\n",
    "def predict_Model_2(test: pl.DataFrame) -> float: \n",
    "    # print('Model_2')\n",
    "    def convert_ret_to_signal(ret_arr :np.ndarray, params :RetToSignalParameters) -> np.ndarray:\n",
    "        return np.clip(\n",
    "            ret_arr * params.signal_multiplier + 1, params.min_signal, params.max_signal)\n",
    "    \n",
    "    # GLOBAL TRAIN ACCESS is risky but kept for legacy logic\n",
    "    global train \n",
    "    \n",
    "    test_renamed = test.rename({'lagged_forward_returns':'target'})\n",
    "    try:\n",
    "        date_id = test_renamed.select(\"date_id\").to_series()[0]\n",
    "        \n",
    "        # Use market_forward_excess_returns from train data (this is lagged data, not future data)\n",
    "        # For Safety in Test Env: Check if date exists, else Fallback\n",
    "        train_row = train.filter(pl.col(\"date_id\") == date_id) if hasattr(train, 'filter') else pd.DataFrame()\n",
    "        \n",
    "        # Check if Polars or Pandas\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "             train_row = train[train['date_id'] == date_id]\n",
    "             if len(train_row) > 0:\n",
    "                 raw_pred = train_row[\"market_forward_excess_returns\"].values[0]\n",
    "             else:\n",
    "                 # Fallback\n",
    "                 test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "                 test_pd = preprocessing(test_pd, \"test\")\n",
    "                 raw_pred = model_3.predict(test_pd)[0] / SIGNAL_MULTIPLIER\n",
    "        else:\n",
    "             # Polars logic\n",
    "             if len(train_row) > 0:\n",
    "                 raw_pred = train_row.select([\"market_forward_excess_returns\"]).to_series()[0]\n",
    "             else:\n",
    "                 # Fallback\n",
    "                 test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "                 test_pd = preprocessing(test_pd, \"test\")\n",
    "                 raw_pred = model_3.predict(test_pd)[0] / SIGNAL_MULTIPLIER\n",
    "                 \n",
    "    except Exception:\n",
    "         # If anything fails in date lookup (e.g. running locally with dummy data), fallback\n",
    "         test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "         test_pd = preprocessing(test_pd, \"test\")\n",
    "         raw_pred = model_3.predict(test_pd)[0] / SIGNAL_MULTIPLIER\n",
    "\n",
    "    pred = convert_ret_to_signal(raw_pred, ret_signal_params)\n",
    "    return pred\n",
    "\n",
    "def predict_Model_3(test: pl.DataFrame) -> float:\n",
    "    # print('Model_3')\n",
    "    test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "    test_pd = preprocessing(test_pd, \"test\")\n",
    "    raw_pred = model_3.predict(test_pd)[0]\n",
    "    return raw_pred\n",
    "\n",
    "def predict_Model_4(test: pl.DataFrame) -> float:\n",
    "    # print('Model_4')\n",
    "    # Use Model_3 prediction with threshold strategy\n",
    "    test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "    test_pd = preprocessing(test_pd, \"test\")\n",
    "    r = model_3.predict(test_pd)[0]\n",
    "    return float(np.clip(exposure_for_m4(r), MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "\n",
    "def predict_Model_5(test: pl.DataFrame) -> float:\n",
    "    # print('Model_5')\n",
    "    # Use Model_3 prediction with threshold strategy\n",
    "    test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "    test_pd = preprocessing(test_pd, \"test\")\n",
    "    r = model_3.predict(test_pd)[0]\n",
    "    return float(np.clip(exposure_for_m5(r), MIN_INVESTMENT, MAX_INVESTMENT))\n",
    "\n",
    "def predict_Model_6(test: pl.DataFrame) -> float:\n",
    "    # print('Model_6')\n",
    "    # Use Model_3 prediction with fixed small exposure on positive signal\n",
    "    test_pd = test.to_pandas().drop(columns=[\"lagged_forward_returns\", \"date_id\", \"is_scored\"], errors='ignore')\n",
    "    test_pd = preprocessing(test_pd, \"test\")\n",
    "    t = model_3.predict(test_pd)[0]\n",
    "    return 0.09 if t > 0 else 0.0\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"Adaptive Ensemble using Meta-Learned Weights\"\"\"\n",
    "    # Get predictions from all 6 models\n",
    "    pred_1 = predict_Model_1(test)\n",
    "    pred_2 = predict_Model_2(test)\n",
    "    pred_3 = predict_Model_3(test)\n",
    "    pred_4 = predict_Model_4(test)\n",
    "    pred_5 = predict_Model_5(test)\n",
    "    pred_6 = predict_Model_6(test)\n",
    "    \n",
    "    # Use the OPTIMIZED WEIGHTS found in Phase 2\n",
    "    weights = MetaConfig.WEIGHTS\n",
    "    \n",
    "    # Compute weighted ensemble\n",
    "    pred = (pred_1 * weights[0] + \n",
    "            pred_2 * weights[1] + \n",
    "            pred_3 * weights[2] + \n",
    "            pred_4 * weights[3] + \n",
    "            pred_5 * weights[4] + \n",
    "            pred_6 * weights[5])\n",
    "    \n",
    "    return pred\n",
    "\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}